{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6412abea",
   "metadata": {},
   "source": [
    "#### Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90beee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import foundation\n",
    "from ai_economist import foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e241ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# from utils import plotting  # plotting utilities for visualizing env. state\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a26f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger\n",
    "import logging.config\n",
    "import yaml\n",
    "\n",
    "with open('configs/logging_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f.read())\n",
    "    logging.config.dictConfig(config)\n",
    "    logging.captureWarnings(True)\n",
    "\n",
    "def get_logger(name: str):\n",
    "    \"\"\"Logs a message\n",
    "    Args:\n",
    "    name(str): name of logger\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    return logger\n",
    "\n",
    "LOG = get_logger('jupyter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8095f",
   "metadata": {},
   "source": [
    "#### Environment  \n",
    "Define environment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9022ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    # ===== SCENARIO CLASS =====\n",
    "    # Which Scenario class to use: the class's name in the Scenario Registry (foundation.scenarios).\n",
    "    # The environment object will be an instance of the Scenario class.\n",
    "    'scenario_name': 'layout_from_file/simple_wood_and_stone',\n",
    "    \n",
    "    # ===== COMPONENTS =====\n",
    "    # Which components to use (specified as list of (\"component_name\", {component_kwargs}) tuples).\n",
    "    #   \"component_name\" refers to the Component class's name in the Component Registry (foundation.components)\n",
    "    #   {component_kwargs} is a dictionary of kwargs passed to the Component class\n",
    "    # The order in which components reset, step, and generate obs follows their listed order below.\n",
    "    'components': [\n",
    "        # (1) Building houses\n",
    "        ('Build', {'skill_dist': \"pareto\", 'payment_max_skill_multiplier': 3}),\n",
    "        # (2) Trading collectible resources\n",
    "        ('ContinuousDoubleAuction', {'max_num_orders': 5}),\n",
    "        # (3) Movement and resource collection\n",
    "        ('Gather', {}),\n",
    "    ],\n",
    "    \n",
    "    # ===== SCENARIO CLASS ARGUMENTS =====\n",
    "    # (optional) kwargs that are added by the Scenario class (i.e. not defined in BaseEnvironment)\n",
    "    'env_layout_file': 'quadrant_25x25_20each_30clump.txt',\n",
    "    'starting_agent_coin': 10,\n",
    "    'fixed_four_skill_and_loc': True,\n",
    "    \n",
    "    # ===== STANDARD ARGUMENTS ======\n",
    "    # kwargs that are used by every Scenario class (i.e. defined in BaseEnvironment)\n",
    "    'n_agents': 2,          # Number of non-planner agents (must be > 1)\n",
    "    'world_size': [25, 25], # [Height, Width] of the env world\n",
    "    'episode_length': 1000, # Number of timesteps per episode\n",
    "    \n",
    "    # In multi-action-mode, the policy selects an action for each action subspace (defined in component code).\n",
    "    # Otherwise, the policy selects only 1 action.\n",
    "    'multi_action_mode_agents': False,\n",
    "    'multi_action_mode_planner': True,\n",
    "    \n",
    "    # When flattening observations, concatenate scalar & vector observations before output.\n",
    "    # Otherwise, return observations with minimal processing.\n",
    "    'flatten_observations': False,\n",
    "    # When Flattening masks, concatenate each action subspace mask into a single array.\n",
    "    # Note: flatten_masks = True is required for masking action logits in the code below.\n",
    "    'flatten_masks': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a312b6",
   "metadata": {},
   "source": [
    "Create environemnt instance using env_config config.  \n",
    "It's equivalent to `gym.make(env_name)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1af167",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = foundation.make_env_instance(**env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8267064",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9490b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "aaa = obs\n",
    "print(len((aaa['p']).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a2ca5",
   "metadata": {},
   "source": [
    "### Info\n",
    "i dictionary degli agenti hanno 24 elementi e come struttura sono uguali.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70414479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4fc73fb2c621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4d887a1",
   "metadata": {},
   "source": [
    "## Agent class and nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d43500ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.seed()\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer \n",
    "\n",
    "# TODO: capire cosa fa `np.array(envs.single_observation_space.shape).prod()`\n",
    "# probabilmente tira fuori la forma dell'np_array di una singola osservazione\n",
    "# -> passare qui come \"envs\" degli environment gia' trasformati in `numpy_env` \n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4b313",
   "metadata": {},
   "source": [
    "###### Actions need to be a dictionary of `id_agent` (obviusly string):`action_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2fcc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state == observation\n",
    "\n",
    "# def sample_random_action(agent, mask, rew = 0):\n",
    "#     \"\"\"\n",
    "#     Sample random UNMASKED action(s) for agent.\n",
    "#     Args:\n",
    "#         agent (ai_economist.foundation.agents.mobiles.BasicMobileAgent):\n",
    "#         mask (array([1.], dtype=float32)): agent action mask\n",
    "#         rew (dict): dictionary containing agent id and reward value FIXME\n",
    "#     \"\"\"\n",
    "#     # Return a list of actions: 1 for each action subspace\n",
    "#     # print(f\"AGENT reward: {rew}\")\n",
    "    \n",
    "#     if agent.multi_action_mode:\n",
    "#         # by default used by the planner\n",
    "#         split_masks = np.split(mask, agent.action_spaces.cumsum()[:-1])\n",
    "#         return [np.random.choice(np.arange(len(m_)), p=m_/m_.sum()) for m_ in split_masks]\n",
    "\n",
    "#     # Return a single action\n",
    "#     else:\n",
    "#         # here agent's training should be implemented\n",
    "#         return np.random.choice(np.arange(agent.action_spaces), p=mask/mask.sum())\n",
    "\n",
    "# def sample_random_actions(env, obs, rew = 0):\n",
    "#     \"\"\"\n",
    "#     Samples random UNMASKED actions for each agent in obs.\n",
    "#     Args:\n",
    "#         env (ai_economist.foundation.scenarios): the used environment\n",
    "#         obs (env.reset()): env.reset()\n",
    "#         rew (): dictionary of agent:reward(float)\n",
    "        \n",
    "#     \"\"\"\n",
    "        \n",
    "#     actions = {\n",
    "#         a_idx: sample_random_action(env.get_agent(a_idx), a_obs['action_mask'], rew[str(a_idx)])\n",
    "#         for a_idx, a_obs in obs.items()\n",
    "#     }\n",
    "\n",
    "#     return actions\n",
    "\n",
    "### TEST\n",
    "# obs = env.reset()\n",
    "# a ctions = sample_random_actions(env, obs)\n",
    "# print(actions)\n",
    "#obs, rew, done, info = env.step(actions)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e431a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_actions(env, obs, rew = 0):\n",
    "    \"\"\"\n",
    "    Returns: dict of actions agent_id : action\n",
    "    If agent then PPO, if planner do nothing.\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    actions = {\n",
    "        a_idx: sample_random_action(env.get_agent(a_idx), a_obs['action_mask'], rew[str(a_idx)])\n",
    "        for a_idx, a_obs in obs.items()\n",
    "    }\n",
    "    \"\"\"\n",
    "    actions = {}\n",
    "    for a_idx, a_obs in obs.items():\n",
    "        \n",
    "        agent = env.get_agent(a_idx)\n",
    "        mask = a_obs['action_mask']\n",
    "        reward = rew[str(a_idx)]\n",
    "\n",
    "        # fixed for the planner\n",
    "        if agent.multi_action_mode and a_idx == 'p':\n",
    "            split_masks = np.split(mask, agent.action_spaces.cumsum()[:-1])\n",
    "            action = [np.random.choice(np.arange(len(m_)), p=m_/m_.sum()) for m_ in split_masks]\n",
    "\n",
    "        # Return a single action -> agent only\n",
    "        else:\n",
    "            # PPO\n",
    "            \n",
    "            action = np.random.choice(np.arange(agent.action_spaces), p=mask/mask.sum())\n",
    "            # LOG.debug(f\"PPO - {a_idx} {action}\")\n",
    "        actions[a_idx] = action\n",
    "    # print(actions)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e19a9b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "def rewards_dictionary(env):   \n",
    "    \"\"\"\n",
    "    Fills and prepares rewards dictionary with all agents/planner ids and sets rewards to None\n",
    "    \n",
    "    Args:\n",
    "        env (ai_economist.foundation.scenarios): the used environment\n",
    "    \n",
    "    Returns:\n",
    "        rewards (dict): agent_id : agent_reward\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    rewards = {}\n",
    "\n",
    "    for a_idx, _ in obs.items():\n",
    "        rewards[str(a_idx)] = None\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "rewards = rewards_dictionary(env)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    \n",
    "    print(f\"Epoch: {i}\")\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        actions = sample_random_actions(env, obs, rewards)\n",
    "        # print(actions)\n",
    "        obs, rew, done, info = env.step(actions)\n",
    "        # print(rew)\n",
    "        rewards = rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8489e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('aie')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e22cfd2707e7b742d4a320dc809ef36135b30e0f8f0258c7992992c90cac853a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
