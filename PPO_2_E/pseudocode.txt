# our implementation
for 1, 2, ..., episodes
    # do a training step

    # create batch
    reset batching environment and get its observation
    reset rollout_buffer
    for 1, 2, ..., train_batch_size>
        get action, action_logprob for all agents in each policy wrt observation
        get new_observation, reward, done from stepping the environment
        save new_observation, reward, done, action, action_logprob in rollout_buffer (it's like a list)
        observation = new_observation
    end

    # train policies
    for all policies
        for each agent in the policy
            get it's rollout_buffer data
            calculate advantages

            for 1, 2, ..., num_sgd_iter
                calculate loss
                optimizer zero_grad
                do back propagation
                optimizer step
            end
        end
    end
end

# rllib implementation - with simplified rollouw workers
for 1, 2, ..., episodes:
    # do a training step
    
    # create batch
    IN PARALLEL EXEC (multi-processing) on all rollouw workers
        # logic here
        reset batching environment and get its observation
        reset rollout_buffer
        counter = 0

        # worker_step = train_batch_size/num_rollout_workers with 
        #   train_batch_size%num_rollout_workers = 0
        #   train_batch_size%sgd_minibatch_size = 0 -> used in training
        for 1, 2, ..., worker_step
            get actions, action_logprob for all agents in each policy* wrt observation
            get new_observation, reward, done from stepping the environment

            if counter module rollout_fragment_length == 0
                # end this episode, start a new one. Set done to True (used in policies)
                reset batching environment and get its observation
                save new_observation, reward, done, action, action_logprob in rollout_buffer
            else
                save new_observation, reward, done, action, action_logprob in rollout_buffer
            end
    	end for
    END PARALLEL EXEC

    join worker's rollout_buffers

    # train policies - single learner
    for all policies **
        for each agent in the policy **
            for train_batch_size // sgd_minibatch_size **
                take from rollout_buffer a minibatch of sgd_minibatch_size size
                compute advantages

                for 1, 2, ..., num_sgd_iter:
                    calculate loss
                    optimizer zero_grad
                    do back propagation
                    optimizer step

                end for
            end for
        end for
    end for

    # update
    update policies in all rollouw workers
end for

*: rollouw workers internal policies (each one has its own policies)
**: trade places of these cycles can improve excecution time depending on how the memory is structured 

Legend
- episodes: Number of episodes to run the training for;
- train_batch_size: total batch size
- num_rollout_workers: number of rollout workers -> used during batching
- rollout_fragment_length: Divide episodes into fragments of this many steps each during rollouts. Sample batches of this size are collected from rollout workers and  combined into a larger batch of "train_batch_size" for learning.
- sgd_minibatch_size: Total SGD batch size across all devices for SGD.
- num_sgd_iter: number of SGD iterations in each outer loop (i.e., number of epochs to execute per train batch)
