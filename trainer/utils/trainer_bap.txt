
    (
        # ðŸ”´ - simple training without any saving/checkpoint/tooling
        # the only weight we care to save during Phase1 is Agent's bc it will
        # be loaded during Phase2 (as paper, to do not discourage exploration.)
        # ðŸš« don't touch - backup
        # for i in range(run_config["general"]["episodes"]):
        #     print(f"== Iteration {i} ==")
        #     # Improve trainerAgents policy's
        #     print("-- PPO Agents --")
        #     result_ppo_agents = trainerAgents.train()
        #     # print(f"{result_ppo_agents['episode_reward_max']}, {result_ppo_agents['episode_reward_min']}, {result_ppo_agents['episode_reward_mean']}, {result_ppo_agents['episode_len_mean']}, {result_ppo_agents['episodes_this_iter']}")
        #     print(pretty_print(result_ppo_agents))
        #     if run_config["general"]["train_planner"]:
        #         # Improve trainerPlanner policy's
        #         print("-- PPO Planner --")
        #         result_ppo_planner = trainerPlanner.train()
        #         # print(f"{result_ppo_planner['episode_reward_max']}, {result_ppo_planner['episode_reward_min']}, {result_ppo_planner['episode_reward_mean']}, {result_ppo_planner['episode_len_mean']}, {result_ppo_planner['episodes_this_iter']}")
        #         print(pretty_print(result_ppo_planner))
        #         # Swap weights to synchronize
        #         trainerAgents.set_weights(
        #             trainerPlanner.get_weights(["planner_policy"]))
        #         trainerPlanner.set_weights(trainerAgents.get_weights(["agent_policy"]))
    )