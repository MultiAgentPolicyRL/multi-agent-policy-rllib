Allocate Nodes = <hpc-c07-node06 hpc-c07-node10 hpc-c07-node09 hpc-c05-node02 hpc-c07-node14 hpc-c07-node05 hpc-c04-node06 hpc-c03-node09 hpc-c03-node17 hpc-c06-node01 hpc-c01-node04 hpc-c01-node13 hpc-c01-node19>
set up ray cluster...


Working with node hpc-c07-node06
first allocate node - use as headnode ...
2022-10-04 19:22:05,190	INFO scripts.py:357 -- Using IP address 192.168.115.58 for this node.
2022-10-04 19:22:05,196	INFO resource_spec.py:212 -- Starting Ray with 103.17 GiB memory available for workers and up to 48.23 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:06,676	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 19:22:06,719	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.58:6383' --redis-password='dd0eb44e-4431-400e-9c7e-aeaf9be962c9'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='dd0eb44e-4431-400e-9c7e-aeaf9be962c9')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c07-node10
then allocate other nodes:  1
node NAME: hpc-c07-node06.unitn.it
node IP: 192.168.115.58
dest IP: 192.168.115.58:6383

Working with node hpc-c07-node09
then allocate other nodes:  2
node NAME: hpc-c07-node10.unitn.it
node IP: 192.168.115.51
dest IP: 192.168.115.58:6383
2022-10-04 19:22:24,895	INFO scripts.py:429 -- Using IP address 192.168.115.51 for this node.
2022-10-04 19:22:24,900	INFO resource_spec.py:212 -- Starting Ray with 171.29 GiB memory available for workers and up to 73.41 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:24,934	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node10.unitn.it

Working with node hpc-c05-node02
then allocate other nodes:  3
node NAME: hpc-c07-node10.unitn.it
node IP: 192.168.115.51
dest IP: 192.168.115.58:6383
2022-10-04 19:22:34,555	INFO scripts.py:429 -- Using IP address 192.168.115.51 for this node.
2022-10-04 19:22:34,560	INFO resource_spec.py:212 -- Starting Ray with 171.24 GiB memory available for workers and up to 73.4 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:34,590	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node10.unitn.it

Working with node hpc-c07-node14
then allocate other nodes:  4
node NAME: hpc-c07-node09.unitn.it
node IP: 192.168.115.50
dest IP: 192.168.115.58:6383
2022-10-04 19:22:45,124	INFO scripts.py:429 -- Using IP address 192.168.115.50 for this node.
2022-10-04 19:22:45,138	INFO resource_spec.py:212 -- Starting Ray with 145.7 GiB memory available for workers and up to 62.45 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:45,177	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node09.unitn.it

Working with node hpc-c07-node05
then allocate other nodes:  5
node NAME: hpc-c07-node09.unitn.it
node IP: 192.168.115.50
dest IP: 192.168.115.58:6383
2022-10-04 19:22:54,789	INFO scripts.py:429 -- Using IP address 192.168.115.50 for this node.
2022-10-04 19:22:54,792	INFO resource_spec.py:212 -- Starting Ray with 145.7 GiB memory available for workers and up to 62.44 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:54,824	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node09.unitn.it

Working with node hpc-c04-node06
then allocate other nodes:  6
node NAME: hpc-c05-node02.unitn.it
node IP: 192.168.115.22
dest IP: 192.168.115.58:6383
2022-10-04 19:23:05,114	INFO scripts.py:429 -- Using IP address 192.168.115.22 for this node.
2022-10-04 19:23:05,117	INFO resource_spec.py:212 -- Starting Ray with 564.16 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:23:05,156	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c05-node02.unitn.it

Working with node hpc-c03-node09
then allocate other nodes:  7
node NAME: hpc-c05-node02.unitn.it
node IP: 192.168.115.22
dest IP: 192.168.115.58:6383
2022-10-04 19:23:14,791	INFO scripts.py:429 -- Using IP address 192.168.115.22 for this node.
2022-10-04 19:23:14,796	INFO resource_spec.py:212 -- Starting Ray with 564.11 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:23:14,830	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c05-node02.unitn.it

Working with node hpc-c03-node17
then allocate other nodes:  8
node NAME: hpc-c07-node14.unitn.it
node IP: 192.168.115.55
dest IP: 192.168.115.58:6383
2022-10-04 19:23:24,965	INFO scripts.py:429 -- Using IP address 192.168.115.55 for this node.
2022-10-04 19:23:24,970	INFO resource_spec.py:212 -- Starting Ray with 170.95 GiB memory available for workers and up to 73.27 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:23:25,002	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node14.unitn.it

Working with node hpc-c06-node01
then allocate other nodes:  9
node NAME: hpc-c07-node14.unitn.it
node IP: 192.168.115.55
dest IP: 192.168.115.58:6383
2022-10-04 19:23:34,454	INFO scripts.py:429 -- Using IP address 192.168.115.55 for this node.
2022-10-04 19:23:34,459	INFO resource_spec.py:212 -- Starting Ray with 170.95 GiB memory available for workers and up to 73.27 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:23:34,490	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node14.unitn.it

Working with node hpc-c01-node04
then allocate other nodes:  10
node NAME: hpc-c07-node05.unitn.it
node IP: 192.168.115.7
dest IP: 192.168.115.58:6383
2022-10-04 19:23:43,656	INFO scripts.py:429 -- Using IP address 192.168.115.58 for this node.
2022-10-04 19:23:43,661	INFO resource_spec.py:212 -- Starting Ray with 110.84 GiB memory available for workers and up to 47.52 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:23:43,702	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node06.unitn.it
2022-10-04 19:23:47,224	INFO scripts.py:429 -- Using IP address 192.168.115.7 for this node.
2022-10-04 19:23:47,230	INFO resource_spec.py:212 -- Starting Ray with 171.09 GiB memory available for workers and up to 73.34 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:23:47,266	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c01-node13
then allocate other nodes:  11
exiting hpc-c07-node05.unitn.it
node NAME: hpc-c07-node05.unitn.it
node IP: 192.168.115.7
dest IP: 192.168.115.58:6383
2022-10-04 19:23:59,636	INFO scripts.py:429 -- Using IP address 192.168.115.7 for this node.
2022-10-04 19:23:59,641	INFO resource_spec.py:212 -- Starting Ray with 171.09 GiB memory available for workers and up to 73.33 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:23:59,671	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c01-node19
then allocate other nodes:  12
node NAME: hpc-c04-node06.unitn.it
node IP: 192.168.115.127
dest IP: 192.168.115.58:6383
2022-10-04 19:24:05,311	INFO scripts.py:429 -- Using IP address 192.168.115.127 for this node.
2022-10-04 19:24:05,315	INFO resource_spec.py:212 -- Starting Ray with 254.49 GiB memory available for workers and up to 109.08 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:24:05,345	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c04-node06.unitn.it
exiting hpc-c07-node05.unitn.it

done, now launching python program
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 19:24:45,924	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 19:24:45,996 seed (final): 27741000
2022-10-04 19:24:46,067	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 19:24:46,396	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 19:25:07,472	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.58: 1.000000}, {CPU: 72.000000}, {memory: 103.173828 GiB}, {object_store_memory: 33.251953 GiB}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-04 19:25:19,062	INFO trainable.py:180 -- _setup took 32.667 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 19:25:19,063	INFO trainable.py:217 -- Getting current IP.
2022-10-04 19:25:45,183	INFO trainable.py:180 -- _setup took 26.045 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 19:25:45,185	INFO trainable.py:217 -- Getting current IP.
2022-10-04 19:25:45,224 Not restoring trainer...
2022-10-04 19:25:45,225 Starting with fresh agent TF weights.
2022-10-04 19:25:45,225 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 19:30:45,271 Iter 1: steps this-iter 4000 total 4000 -> 3/5000 episodes done
2022-10-04 19:30:45,277 custom_metrics: {}
date: 2022-10-04_19-30-45
done: false
episode_len_mean: 1000.0
episode_reward_max: 90.34790623573369
episode_reward_mean: 78.41513167983236
episode_reward_min: 63.77587941868688
episodes_this_iter: 3
episodes_total: 3
experiment_id: f8c73c9d131a44c9ba165c3e0aa2e657
hostname: hpc-c07-node06.unitn.it
info:
  grad_time_ms: 253709.376
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7747539281845093
      entropy_coeff: 0.02500000037252903
      kl: 0.09983952343463898
      model: {}
      policy_loss: -0.14767584204673767
      total_loss: -0.17071211338043213
      vf_explained_var: 0.87552410364151
      vf_loss: 0.4266514778137207
  load_time_ms: 1954.629
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 27226.943
  update_time_ms: 14634.392
iterations_since_restore: 1
node_ip: 192.168.115.58
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 88.13777777777779
  ram_util_percent: 38.641975308641975
pid: 65842
policy_reward_max:
  agent_policy: 24.973326536732078
  planner_policy: 21.19958432085763
policy_reward_mean:
  agent_policy: 15.142776978348044
  planner_policy: 17.844023766440618
policy_reward_min:
  agent_policy: -1.9783118582457848
  planner_policy: 13.832486978463107
sampler_perf:
  mean_env_wait_ms: 3.103720821561094
  mean_inference_ms: 5.143689796702531
  mean_processing_ms: 1.1909431033343716
time_since_restore: 297.9586925506592
time_this_iter_s: 297.9586925506592
time_total_s: 297.9586925506592
timestamp: 1664904645
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 19:30:45,456 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 3
=>> PBS: job killed: walltime 602 exceeded limit 600
*** Aborted at 1664904722 (unix time) try "date -d @1664904722" if you are using GNU date ***
PC: @                0x0 (unknown)
2022-10-04 19:32:02,185	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
2022-10-04 19:32:02,187	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
*** SIGTERM (@0xd5b) received by PID 65842 (TID 0x2b06caef23c0) from PID 3419; stack trace: ***
    @     0x2b06cb0ef630 (unknown)
    @     0x2b06cbafdbf9 syscall
    @     0x2b06fe739479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2b06fe738ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2b06fe7360e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b06fe7365e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b06fd7b040b tensorflow::DirectSession::RunInternal()
    @     0x2b06fd7b1829 tensorflow::DirectSession::Run()
    @     0x2b06fd79a134 tensorflow::DirectSession::Run()
    @     0x2b06edab58c2 tensorflow::SessionRef::Run()
    @     0x2b06edf10f5e TF_Run_Helper()
    @     0x2b06edf11c28 TF_SessionRun
    @     0x2b06edaaf01f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b06edaaf0c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b0721392274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b072137bc98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
