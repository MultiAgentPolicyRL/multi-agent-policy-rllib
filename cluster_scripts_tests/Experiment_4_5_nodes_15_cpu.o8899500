Allocate Nodes = <hpc-c11-node15 hpc-c10-node01 hpc-c11-node22>
set up ray cluster...


Working with node hpc-c11-node15
first allocate node - use as headnode ...
2022-10-04 19:02:52,103	INFO scripts.py:357 -- Using IP address 192.168.115.81 for this node.
2022-10-04 19:02:52,114	INFO resource_spec.py:212 -- Starting Ray with 766.41 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:02:53,083	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 19:02:53,118	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.81:6379' --redis-password='7fa7e33c-0512-4856-81d0-d2ff35183f9d'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='7fa7e33c-0512-4856-81d0-d2ff35183f9d')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c10-node01
then allocate other nodes:  1
node NAME: hpc-c10-node01.unitn.it
node IP: 192.168.115.134
dest IP: 192.168.115.81:6379

Working with node hpc-c11-node22
then allocate other nodes:  2
node NAME: hpc-c10-node01.unitn.it
node IP: 192.168.115.134
dest IP: 192.168.115.81:6379

done, now launching python program
2022-10-04 19:03:31,931	INFO scripts.py:429 -- Using IP address 192.168.115.134 for this node.
2022-10-04 19:03:31,936	INFO resource_spec.py:212 -- Starting Ray with 277.83 GiB memory available for workers and up to 119.08 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:03:31,972	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c10-node01.unitn.it
2022-10-04 19:03:32,421	INFO scripts.py:429 -- Using IP address 192.168.115.134 for this node.
2022-10-04 19:03:32,425	INFO resource_spec.py:212 -- Starting Ray with 277.83 GiB memory available for workers and up to 119.08 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:03:32,448	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c10-node01.unitn.it
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 19:04:39,811	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 19:04:39,837 seed (final): 26535000
2022-10-04 19:04:39,896	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 19:04:40,110	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 19:04:53,597	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.81: 1.000000}, {CPU: 96.000000}, {memory: 766.406250 GiB}, {object_store_memory: 128.515625 GiB}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-04 19:05:02,104	INFO trainable.py:180 -- _setup took 21.996 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 19:05:02,105	INFO trainable.py:217 -- Getting current IP.
2022-10-04 19:05:16,741	INFO trainable.py:180 -- _setup took 14.555 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 19:05:16,741	INFO trainable.py:217 -- Getting current IP.
2022-10-04 19:05:16,744 Not restoring trainer...
2022-10-04 19:05:16,744 Starting with fresh agent TF weights.
2022-10-04 19:05:16,744 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 19:08:38,006 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-04 19:08:38,011 custom_metrics: {}
date: 2022-10-04_19-08-37
done: false
episode_len_mean: 1000.0
episode_reward_max: 220.60671965274489
episode_reward_mean: 188.47528257639993
episode_reward_min: 155.97479505992135
episodes_this_iter: 4
episodes_total: 4
experiment_id: cdf4d17aa5f84ecb93914f01615f7e21
hostname: hpc-c11-node15.unitn.it
info:
  grad_time_ms: 144908.509
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9472432136535645
      entropy_coeff: 0.02500000037252903
      kl: 0.1196160614490509
      model: {}
      policy_loss: -0.15623624622821808
      total_loss: -0.15704528987407684
      vf_explained_var: 0.8631814122200012
      vf_loss: 0.9574407339096069
  load_time_ms: 1427.161
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 45990.884
  update_time_ms: 7704.516
iterations_since_restore: 1
node_ip: 192.168.115.81
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 65.52769784172662
  ram_util_percent: 5.5071942446043165
pid: 87806
policy_reward_max:
  agent_policy: 124.02724291484044
  planner_policy: 59.4661632920315
policy_reward_mean:
  agent_policy: 37.88692603277336
  planner_policy: 36.92757844530749
policy_reward_min:
  agent_policy: -3.3555537411955454
  planner_policy: 21.864558277782272
sampler_perf:
  mean_env_wait_ms: 2.165364837837124
  mean_inference_ms: 2.933142901300967
  mean_processing_ms: 0.6555936623668146
time_since_restore: 200.3064820766449
time_this_iter_s: 200.3064820766449
time_total_s: 200.3064820766449
timestamp: 1664903317
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 19:08:38,185 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
2022-10-04 19:11:06,164 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-04 19:11:06,169 custom_metrics: {}
date: 2022-10-04_19-11-06
done: false
episode_len_mean: 1000.0
episode_reward_max: 244.52027992927407
episode_reward_mean: 186.0132884111998
episode_reward_min: 155.97479505992135
episodes_this_iter: 4
episodes_total: 8
experiment_id: cdf4d17aa5f84ecb93914f01615f7e21
hostname: hpc-c11-node15.unitn.it
info:
  grad_time_ms: 140288.891
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8797553777694702
      entropy_coeff: 0.02500000037252903
      kl: 0.14516955614089966
      model: {}
      policy_loss: -0.17388701438903809
      total_loss: -0.18096758425235748
      vf_explained_var: 0.9194316267967224
      vf_loss: 0.7982658743858337
  load_time_ms: 1221.06
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 28605.516
  update_time_ms: 3855.529
iterations_since_restore: 2
node_ip: 192.168.115.81
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 65.7427027027027
  ram_util_percent: 5.4972972972972975
pid: 87806
policy_reward_max:
  agent_policy: 124.02724291484044
  planner_policy: 62.47771919984916
policy_reward_mean:
  agent_policy: 36.63226757172939
  planner_policy: 39.484218124282854
policy_reward_min:
  agent_policy: -3.3555537411955454
  planner_policy: 21.864558277782272
sampler_perf:
  mean_env_wait_ms: 2.13753066643329
  mean_inference_ms: 2.863936813584881
  mean_processing_ms: 0.6545850093315124
time_since_restore: 348.26796293258667
time_this_iter_s: 147.96148085594177
time_total_s: 348.26796293258667
timestamp: 1664903466
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

-- PPO Agents -- Steps done: 8
=>> PBS: job killed: walltime 606 exceeded limit 600
*** Aborted at 1664903576 (unix time) try "date -d @1664903576" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xd4b) received by PID 87806 (TID 0x2b08488ab3c0) from PID 3403; stack trace: ***
    @     0x2b0848aa9630 (unknown)
    @     0x2b08494b7e29 syscall
    @     0x2b08914fd479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2b08914fcab9 nsync::nsync_sem_wait_with_cancel_()
2022-10-04 19:12:56,927	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
2022-10-04 19:12:56,930	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
    @     0x2b08914fa0e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b08914fa5e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b089057440b tensorflow::DirectSession::RunInternal()
    @     0x2b0890575829 tensorflow::DirectSession::Run()
    @     0x2b089055e134 tensorflow::DirectSession::Run()
    @     0x2b08808798c2 tensorflow::SessionRef::Run()
    @     0x2b0880cd4f5e TF_Run_Helper()
    @     0x2b0880cd5c28 TF_SessionRun
    @     0x2b088087301f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b08808730c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b08b4156274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b08b413fc98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
