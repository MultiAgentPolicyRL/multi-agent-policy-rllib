Allocate Nodes = <hpc-g04-node02 hpc-g04-node01 hpc-g01-node02 hpc-g02-node02>
set up ray cluster...


Working with node hpc-g04-node02
first allocate node - use as headnode ...
2022-10-05 11:22:36,302	INFO scripts.py:357 -- Using IP address 192.168.115.44 for this node.
2022-10-05 11:22:36,306	INFO resource_spec.py:212 -- Starting Ray with 426.86 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:22:37,454	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-05 11:22:37,496	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.44:3679' --redis-password='56959f24-a56e-4798-a433-6668b807e91b'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='56959f24-a56e-4798-a433-6668b807e91b')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-g04-node01
then allocate other nodes:  1
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.44:3679
2022-10-05 11:22:46,752	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-05 11:22:46,755	INFO resource_spec.py:212 -- Starting Ray with 529.44 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:22:46,779	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

Working with node hpc-g01-node02
then allocate other nodes:  2
node NAME: hpc-g01-node02.unitn.it
node IP: 192.168.115.93
dest IP: 192.168.115.44:3679
2022-10-05 11:22:56,179	INFO scripts.py:429 -- Using IP address 192.168.115.93 for this node.
2022-10-05 11:22:56,181	INFO resource_spec.py:212 -- Starting Ray with 173.29 GiB memory available for workers and up to 74.28 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:22:56,182	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 55419244544 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2022-10-05 11:22:56,199	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g01-node02.unitn.it

Working with node hpc-g02-node02
then allocate other nodes:  3
node NAME: hpc-g02-node02.unitn.it
node IP: 192.168.115.10
dest IP: 192.168.115.44:3679
2022-10-05 11:23:06,417	INFO scripts.py:429 -- Using IP address 192.168.115.10 for this node.
2022-10-05 11:23:06,420	INFO resource_spec.py:212 -- Starting Ray with 218.46 GiB memory available for workers and up to 93.64 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:23:06,420	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 68429537280 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2022-10-05 11:23:06,439	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g02-node02.unitn.it

done, now launching python program
Inside covid19_components.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
Inside covid19_env.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
2022-10-05 11:24:25,118	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-05 11:24:25,145 seed (final): 19785000
2022-10-05 11:24:25,204	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-05 11:24:25,445	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-05 11:24:37,981	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.44: 1.000000}, {object_store_memory: 128.515625 GiB}, {CPU: 72.000000}, {memory: 426.855469 GiB}, {GPU: 1.000000}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-05 11:24:46,994	INFO trainable.py:180 -- _setup took 21.550 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 11:24:46,994	INFO trainable.py:217 -- Getting current IP.
2022-10-05 11:24:59,884	INFO trainable.py:180 -- _setup took 12.799 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 11:24:59,884	INFO trainable.py:217 -- Getting current IP.
2022-10-05 11:24:59,953 Not restoring trainer...
2022-10-05 11:24:59,954 Starting with fresh agent TF weights.
2022-10-05 11:24:59,954 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-05 11:28:35,503 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-05 11:28:35,508 custom_metrics: {}
date: 2022-10-05_11-28-35
done: false
episode_len_mean: 1000.0
episode_reward_max: 322.5399621118979
episode_reward_mean: 295.2096480253779
episode_reward_min: 257.73385406756836
episodes_this_iter: 4
episodes_total: 4
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 154808.324
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 2.055053472518921
      entropy_coeff: 0.02500000037252903
      kl: 0.12114657461643219
      model: {}
      policy_loss: -0.180845707654953
      total_loss: -0.15438899397850037
      vf_explained_var: 0.8821629881858826
      vf_loss: 1.556660771369934
  load_time_ms: 1507.932
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 50665.288
  update_time_ms: 7377.238
iterations_since_restore: 1
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 61.057142857142864
  gpu_util_percent0: 0.0
  ram_util_percent: 23.332752613240416
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 108.36011498772727
  planner_policy: 98.49670889471454
policy_reward_mean:
  agent_policy: 52.748135716647724
  planner_policy: 84.21710515878655
policy_reward_min:
  agent_policy: 12.369485903906726
  planner_policy: 57.38248944351155
sampler_perf:
  mean_env_wait_ms: 2.3075641244128606
  mean_inference_ms: 2.907499380554931
  mean_processing_ms: 0.6826428518719461
time_since_restore: 214.64739298820496
time_this_iter_s: 214.64739298820496
time_total_s: 214.64739298820496
timestamp: 1664962115
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-05 11:28:35,699 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
2022-10-05 11:31:13,160 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-05 11:31:13,165 custom_metrics: {}
date: 2022-10-05_11-31-13
done: false
episode_len_mean: 1000.0
episode_reward_max: 372.4495619728708
episode_reward_mean: 276.29953947863174
episode_reward_min: 76.35164674035195
episodes_this_iter: 4
episodes_total: 8
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 149827.811
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9247002601623535
      entropy_coeff: 0.02500000037252903
      kl: 0.13765384256839752
      model: {}
      policy_loss: -0.19560639560222626
      total_loss: -0.19702953100204468
      vf_explained_var: 0.9373769760131836
      vf_loss: 0.9338883757591248
  load_time_ms: 1311.486
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 31047.521
  update_time_ms: 3692.534
iterations_since_restore: 2
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 45.20885416666666
  gpu_util_percent0: 0.0
  ram_util_percent: 21.27135416666667
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 140.08702184598926
  planner_policy: 118.51265536426395
policy_reward_mean:
  agent_policy: 49.507764512927125
  planner_policy: 78.26848142692252
policy_reward_min:
  agent_policy: 7.674316568075659
  planner_policy: 26.14824863133807
sampler_perf:
  mean_env_wait_ms: 2.2599438307185413
  mean_inference_ms: 2.84897055228208
  mean_processing_ms: 0.6825390677354307
time_since_restore: 372.0906331539154
time_this_iter_s: 157.44324016571045
time_total_s: 372.0906331539154
timestamp: 1664962273
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

-- PPO Agents -- Steps done: 8
2022-10-05 11:33:44,658 Iter 3: steps this-iter 4000 total 12000 -> 12/5000 episodes done
2022-10-05 11:33:44,664 custom_metrics: {}
date: 2022-10-05_11-33-44
done: false
episode_len_mean: 1000.0
episode_reward_max: 372.4495619728708
episode_reward_mean: 275.72360433304084
episode_reward_min: 76.35164674035195
episodes_this_iter: 4
episodes_total: 12
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 146504.368
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9087927341461182
      entropy_coeff: 0.02500000037252903
      kl: 0.1535077691078186
      model: {}
      policy_loss: -0.19314396381378174
      total_loss: -0.19989623129367828
      vf_explained_var: 0.946851372718811
      vf_loss: 0.8193508386611938
  load_time_ms: 1300.452
  num_steps_sampled: 12000
  num_steps_trained: 48000
  sample_time_ms: 24128.67
  update_time_ms: 2464.762
iterations_since_restore: 3
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.42108108108108
  gpu_util_percent0: 0.0
  ram_util_percent: 18.62486486486486
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 140.08702184598926
  planner_policy: 127.23120265847423
policy_reward_mean:
  agent_policy: 47.93108057670528
  planner_policy: 83.999282026219
policy_reward_min:
  agent_policy: 5.577281084768025
  planner_policy: 26.14824863133807
sampler_perf:
  mean_env_wait_ms: 2.207709183428267
  mean_inference_ms: 2.7816029089341137
  mean_processing_ms: 0.6733826536803535
time_since_restore: 523.5665311813354
time_this_iter_s: 151.47589802742004
time_total_s: 523.5665311813354
timestamp: 1664962424
timesteps_since_restore: 12000
timesteps_this_iter: 4000
timesteps_total: 12000
training_iteration: 3

-- PPO Agents -- Steps done: 12
2022-10-05 11:36:13,271 Iter 4: steps this-iter 4000 total 16000 -> 16/5000 episodes done
2022-10-05 11:36:13,276 custom_metrics: {}
date: 2022-10-05_11-36-13
done: false
episode_len_mean: 1000.0
episode_reward_max: 420.4407883587105
episode_reward_mean: 293.317261307915
episode_reward_min: 76.35164674035195
episodes_this_iter: 4
episodes_total: 16
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 144127.14
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9341065883636475
      entropy_coeff: 0.02500000037252903
      kl: 0.15634945034980774
      model: {}
      policy_loss: -0.20308732986450195
      total_loss: -0.21521160006523132
      vf_explained_var: 0.9646715521812439
      vf_loss: 0.724567711353302
  load_time_ms: 1252.502
  num_steps_sampled: 16000
  num_steps_trained: 64000
  sample_time_ms: 20705.977
  update_time_ms: 1850.905
iterations_since_restore: 4
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.332596685082876
  gpu_util_percent0: 0.0
  ram_util_percent: 18.692817679558008
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 140.08702184598926
  planner_policy: 160.89280487972948
policy_reward_mean:
  agent_policy: 49.746192707865795
  planner_policy: 94.33249047645143
policy_reward_min:
  agent_policy: 5.577281084768025
  planner_policy: 26.14824863133807
sampler_perf:
  mean_env_wait_ms: 2.1689951548242505
  mean_inference_ms: 2.7284813882374417
  mean_processing_ms: 0.6657299578790628
time_since_restore: 672.1553001403809
time_this_iter_s: 148.5887689590454
time_total_s: 672.1553001403809
timestamp: 1664962573
timesteps_since_restore: 16000
timesteps_this_iter: 4000
timesteps_total: 16000
training_iteration: 4

-- PPO Agents -- Steps done: 16
2022-10-05 11:38:41,710 Iter 5: steps this-iter 4000 total 20000 -> 20/5000 episodes done
2022-10-05 11:38:41,717 custom_metrics: {}
date: 2022-10-05_11-38-41
done: false
episode_len_mean: 1000.0
episode_reward_max: 420.4407883587105
episode_reward_mean: 290.5554937976134
episode_reward_min: 76.35164674035195
episodes_this_iter: 4
episodes_total: 20
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 142571.279
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8513177633285522
      entropy_coeff: 0.02500000037252903
      kl: 0.16921690106391907
      model: {}
      policy_loss: -0.20796312391757965
      total_loss: -0.22458402812480927
      vf_explained_var: 0.9688227772712708
      vf_loss: 0.5932410955429077
  load_time_ms: 1242.909
  num_steps_sampled: 20000
  num_steps_trained: 80000
  sample_time_ms: 18695.576
  update_time_ms: 1482.536
iterations_since_restore: 5
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 41.84364640883978
  gpu_util_percent0: 0.0
  ram_util_percent: 19.34696132596685
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 140.08702184598926
  planner_policy: 162.6988902740174
policy_reward_mean:
  agent_policy: 48.276205265594776
  planner_policy: 97.45067273523375
policy_reward_min:
  agent_policy: -5.185449782256148
  planner_policy: 26.14824863133807
sampler_perf:
  mean_env_wait_ms: 2.1398411709437424
  mean_inference_ms: 2.690861911209147
  mean_processing_ms: 0.6602361247553127
time_since_restore: 820.5229671001434
time_this_iter_s: 148.36766695976257
time_total_s: 820.5229671001434
timestamp: 1664962721
timesteps_since_restore: 20000
timesteps_this_iter: 4000
timesteps_total: 20000
training_iteration: 5

-- PPO Agents -- Steps done: 20
2022-10-05 11:41:14,993 Iter 6: steps this-iter 4000 total 24000 -> 23/5000 episodes done
2022-10-05 11:41:15,001 custom_metrics: {}
date: 2022-10-05_11-41-14
done: false
episode_len_mean: 1000.0
episode_reward_max: 420.4407883587105
episode_reward_mean: 257.0414142847242
episode_reward_min: 58.28324242655418
episodes_this_iter: 3
episodes_total: 23
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 142245.482
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.663780927658081
      entropy_coeff: 0.02500000037252903
      kl: 0.18206238746643066
      model: {}
      policy_loss: -0.21359166502952576
      total_loss: -0.23466521501541138
      vf_explained_var: 0.9672116637229919
      vf_loss: 0.4104193449020386
  load_time_ms: 1231.67
  num_steps_sampled: 24000
  num_steps_trained: 96000
  sample_time_ms: 17468.639
  update_time_ms: 1237.194
iterations_since_restore: 6
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 62.332258064516125
  gpu_util_percent0: 0.0
  ram_util_percent: 22.676881720430107
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 140.08702184598926
  planner_policy: 162.6988902740174
policy_reward_mean:
  agent_policy: 41.74911051945241
  planner_policy: 90.04497220691408
policy_reward_min:
  agent_policy: -10.085397099943945
  planner_policy: 21.208269050734824
sampler_perf:
  mean_env_wait_ms: 2.0988403478555697
  mean_inference_ms: 2.6370565006506346
  mean_processing_ms: 0.654319259873329
time_since_restore: 973.6998288631439
time_this_iter_s: 153.1768617630005
time_total_s: 973.6998288631439
timestamp: 1664962874
timesteps_since_restore: 24000
timesteps_this_iter: 4000
timesteps_total: 24000
training_iteration: 6

-- PPO Agents -- Steps done: 23
2022-10-05 11:43:52,205 Iter 7: steps this-iter 4000 total 28000 -> 27/5000 episodes done
2022-10-05 11:43:52,210 custom_metrics: {}
date: 2022-10-05_11-43-52
done: false
episode_len_mean: 1000.0
episode_reward_max: 420.4407883587105
episode_reward_mean: 265.12627132449086
episode_reward_min: 58.28324242655418
episodes_this_iter: 4
episodes_total: 27
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 142592.509
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.840212345123291
      entropy_coeff: 0.02500000037252903
      kl: 0.18349716067314148
      model: {}
      policy_loss: -0.22065535187721252
      total_loss: -0.2378499060869217
      vf_explained_var: 0.9734009504318237
      vf_loss: 0.5762155652046204
  load_time_ms: 1215.687
  num_steps_sampled: 28000
  num_steps_trained: 112000
  sample_time_ms: 16592.107
  update_time_ms: 1062.172
iterations_since_restore: 7
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 53.73560209424084
  gpu_util_percent0: 0.0
  ram_util_percent: 23.157068062827236
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 114.1800220801582
  planner_policy: 162.6988902740174
policy_reward_mean:
  agent_policy: 41.459463438388255
  planner_policy: 99.28841757093717
policy_reward_min:
  agent_policy: -10.085397099943945
  planner_policy: 21.208269050734824
sampler_perf:
  mean_env_wait_ms: 2.059188048876006
  mean_inference_ms: 2.5797982388139795
  mean_processing_ms: 0.6471722489889401
time_since_restore: 1130.8794944286346
time_this_iter_s: 157.17966556549072
time_total_s: 1130.8794944286346
timestamp: 1664963032
timesteps_since_restore: 28000
timesteps_this_iter: 4000
timesteps_total: 28000
training_iteration: 7

-- PPO Agents -- Steps done: 27
2022-10-05 11:46:25,960 Iter 8: steps this-iter 4000 total 32000 -> 31/5000 episodes done
2022-10-05 11:46:25,965 custom_metrics: {}
date: 2022-10-05_11-46-25
done: false
episode_len_mean: 1000.0
episode_reward_max: 420.4407883587105
episode_reward_mean: 246.71661139846947
episode_reward_min: 15.52268650889329
episodes_this_iter: 4
episodes_total: 31
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 142548.573
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.743186593055725
      entropy_coeff: 0.02500000037252903
      kl: 0.1966201812028885
      model: {}
      policy_loss: -0.20239396393299103
      total_loss: -0.22159360349178314
      vf_explained_var: 0.9727442264556885
      vf_loss: 0.4876003563404083
  load_time_ms: 1199.433
  num_steps_sampled: 32000
  num_steps_trained: 128000
  sample_time_ms: 15812.527
  update_time_ms: 930.471
iterations_since_restore: 8
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.884491978609624
  gpu_util_percent0: 0.0
  ram_util_percent: 18.733689839572193
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 114.1800220801582
  planner_policy: 179.08293309876063
policy_reward_mean:
  agent_policy: 36.78244932328095
  planner_policy: 99.58681410534538
policy_reward_min:
  agent_policy: -11.344934514852469
  planner_policy: 21.208269050734824
sampler_perf:
  mean_env_wait_ms: 2.038813361148733
  mean_inference_ms: 2.5455077402726083
  mean_processing_ms: 0.6438013744842495
time_since_restore: 1284.6083548069
time_this_iter_s: 153.72886037826538
time_total_s: 1284.6083548069
timestamp: 1664963185
timesteps_since_restore: 32000
timesteps_this_iter: 4000
timesteps_total: 32000
training_iteration: 8

-- PPO Agents -- Steps done: 31
2022-10-05 11:48:59,537 Iter 9: steps this-iter 4000 total 36000 -> 35/5000 episodes done
2022-10-05 11:48:59,543 custom_metrics: {}
date: 2022-10-05_11-48-59
done: false
episode_len_mean: 1000.0
episode_reward_max: 404.88834326132496
episode_reward_mean: 239.1438374529867
episode_reward_min: 15.52268650889329
episodes_this_iter: 4
episodes_total: 35
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 142450.901
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7926642894744873
      entropy_coeff: 0.02500000037252903
      kl: 0.20793302357196808
      model: {}
      policy_loss: -0.22088801860809326
      total_loss: -0.23787608742713928
      vf_explained_var: 0.973851203918457
      vf_loss: 0.5565709471702576
  load_time_ms: 1193.088
  num_steps_sampled: 36000
  num_steps_trained: 144000
  sample_time_ms: 15239.527
  update_time_ms: 827.99
iterations_since_restore: 9
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.781621621621625
  gpu_util_percent0: 0.0
  ram_util_percent: 18.612432432432428
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 114.1800220801582
  planner_policy: 201.83586855448678
policy_reward_mean:
  agent_policy: 33.547687325369644
  planner_policy: 104.95308815150784
policy_reward_min:
  agent_policy: -11.344934514852469
  planner_policy: 21.208269050734824
sampler_perf:
  mean_env_wait_ms: 2.029449878735021
  mean_inference_ms: 2.527712994311801
  mean_processing_ms: 0.6435696201360201
time_since_restore: 1438.1385259628296
time_this_iter_s: 153.53017115592957
time_total_s: 1438.1385259628296
timestamp: 1664963339
timesteps_since_restore: 36000
timesteps_this_iter: 4000
timesteps_total: 36000
training_iteration: 9

-- PPO Agents -- Steps done: 35
2022-10-05 11:51:26,392 Iter 10: steps this-iter 4000 total 40000 -> 39/5000 episodes done
2022-10-05 11:51:26,398 custom_metrics: {}
date: 2022-10-05_11-51-26
done: false
episode_len_mean: 1000.0
episode_reward_max: 404.88834326132496
episode_reward_mean: 233.27870048100817
episode_reward_min: 15.52268650889329
episodes_this_iter: 4
episodes_total: 39
experiment_id: a9873abd3271412a80ce348ebdddefd4
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 141729.328
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.692264199256897
      entropy_coeff: 0.02500000037252903
      kl: 0.21276472508907318
      model: {}
      policy_loss: -0.2156330645084381
      total_loss: -0.22562772035598755
      vf_explained_var: 0.9739786386489868
      vf_loss: 0.646239161491394
  load_time_ms: 1186.249
  num_steps_sampled: 40000
  num_steps_trained: 160000
  sample_time_ms: 14756.229
  update_time_ms: 746.505
iterations_since_restore: 10
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.9567415730337
  gpu_util_percent0: 0.0
  ram_util_percent: 18.677528089887637
  vram_util_percent0: 0.018997524752475244
pid: 28033
policy_reward_max:
  agent_policy: 114.1800220801582
  planner_policy: 201.83586855448678
policy_reward_mean:
  agent_policy: 31.270484204631224
  planner_policy: 108.19676366248322
policy_reward_min:
  agent_policy: -11.344934514852469
  planner_policy: 21.208269050734824
sampler_perf:
  mean_env_wait_ms: 2.0249647581162002
  mean_inference_ms: 2.5134607128425084
  mean_processing_ms: 0.6448338902325513
time_since_restore: 1584.9641983509064
time_this_iter_s: 146.82567238807678
time_total_s: 1584.9641983509064
timestamp: 1664963486
timesteps_since_restore: 40000
timesteps_this_iter: 4000
timesteps_total: 40000
training_iteration: 10

-- PPO Agents -- Steps done: 39
=>> PBS: job killed: walltime 1822 exceeded limit 1800
*** Aborted at 1664963575 (unix time) try "date -d @1664963575" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xfe7c) received by PID 28033 (TID 0x2ae6004d23c0) from PID 65148; stack trace: ***
    @     0x2ae6006cb630 (unknown)
    @     0x2ae6010d9bf9 syscall
2022-10-05 11:52:55,727	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
2022-10-05 11:52:55,729	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
    @     0x2ae649123479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2ae649122ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2ae6491200e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2ae6491205e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2ae64819a40b tensorflow::DirectSession::RunInternal()
    @     0x2ae64819b829 tensorflow::DirectSession::Run()
    @     0x2ae648184134 tensorflow::DirectSession::Run()
    @     0x2ae63849f8c2 tensorflow::SessionRef::Run()
    @     0x2ae6388faf5e TF_Run_Helper()
    @     0x2ae6388fbc28 TF_SessionRun
    @     0x2ae63849901f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2ae6384990c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2ae66c015274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2ae66bffec98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
