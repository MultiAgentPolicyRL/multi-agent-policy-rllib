Allocate Nodes = <hpc-c10-node01 hpc-c07-node12 hpc-g04-node01 hpc-c03-node24 hpc-c05-node03 hpc-c07-node15 hpc-c03-node12 hpc-c11-node07 hpc-c11-node08 hpc-c11-node21>
set up ray cluster...


Working with node hpc-c10-node01
first allocate node - use as headnode ...
2022-10-04 19:21:09,305	INFO scripts.py:357 -- Using IP address 192.168.115.134 for this node.
2022-10-04 19:21:09,312	INFO resource_spec.py:212 -- Starting Ray with 260.69 GiB memory available for workers and up to 115.72 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:21:10,233	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 19:21:10,262	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.134:6380' --redis-password='211224b0-51ce-46f2-bf2a-544ad6074ad5'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='211224b0-51ce-46f2-bf2a-544ad6074ad5')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c07-node12
then allocate other nodes:  1
node NAME: hpc-c07-node12.unitn.it
node IP: 192.168.115.53
dest IP: 192.168.115.134:6380

Working with node hpc-g04-node01
then allocate other nodes:  2
node NAME: hpc-c07-node12.unitn.it
node IP: 192.168.115.53
dest IP: 192.168.115.134:6380
2022-10-04 19:21:33,797	INFO scripts.py:429 -- Using IP address 192.168.115.53 for this node.
2022-10-04 19:21:33,800	INFO resource_spec.py:212 -- Starting Ray with 131.69 GiB memory available for workers and up to 56.45 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:21:33,830	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node12.unitn.it
2022-10-04 19:21:35,038	INFO scripts.py:429 -- Using IP address 192.168.115.53 for this node.
2022-10-04 19:21:35,042	INFO resource_spec.py:212 -- Starting Ray with 131.69 GiB memory available for workers and up to 56.45 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:21:35,059	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c03-node24
then allocate other nodes:  3
exiting hpc-c07-node12.unitn.it
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.134:6380
2022-10-04 19:21:41,090	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-04 19:21:41,093	INFO resource_spec.py:212 -- Starting Ray with 529.35 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:21:41,113	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

Working with node hpc-c05-node03
then allocate other nodes:  4
node NAME: hpc-c03-node24.unitn.it
node IP: 192.168.115.12
dest IP: 192.168.115.134:6380
2022-10-04 19:21:51,331	INFO scripts.py:429 -- Using IP address 192.168.115.12 for this node.
2022-10-04 19:21:51,334	INFO resource_spec.py:212 -- Starting Ray with 232.52 GiB memory available for workers and up to 99.65 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:21:51,368	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c03-node24.unitn.it

Working with node hpc-c07-node15
then allocate other nodes:  5
node NAME: hpc-c05-node03.unitn.it
node IP: 192.168.115.60
dest IP: 192.168.115.134:6380
2022-10-04 19:22:03,506	INFO scripts.py:429 -- Using IP address 192.168.115.60 for this node.
2022-10-04 19:22:03,509	INFO resource_spec.py:212 -- Starting Ray with 557.62 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:03,543	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c05-node03.unitn.it

Working with node hpc-c03-node12
then allocate other nodes:  6
node NAME: hpc-c05-node03.unitn.it
node IP: 192.168.115.60
dest IP: 192.168.115.134:6380
2022-10-04 19:22:12,200	INFO scripts.py:429 -- Using IP address 192.168.115.60 for this node.
2022-10-04 19:22:12,203	INFO resource_spec.py:212 -- Starting Ray with 557.62 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:12,236	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c05-node03.unitn.it

Working with node hpc-c11-node07
then allocate other nodes:  7
node NAME: hpc-c07-node15.unitn.it
node IP: 192.168.115.100
dest IP: 192.168.115.134:6380
2022-10-04 19:22:21,423	INFO scripts.py:429 -- Using IP address 192.168.115.100 for this node.
2022-10-04 19:22:21,427	INFO resource_spec.py:212 -- Starting Ray with 174.46 GiB memory available for workers and up to 74.79 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:21,457	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node15.unitn.it

Working with node hpc-c11-node08
then allocate other nodes:  8
node NAME: hpc-c07-node15.unitn.it
node IP: 192.168.115.100
dest IP: 192.168.115.134:6380
2022-10-04 19:22:31,079	INFO scripts.py:429 -- Using IP address 192.168.115.100 for this node.
2022-10-04 19:22:31,083	INFO resource_spec.py:212 -- Starting Ray with 174.46 GiB memory available for workers and up to 74.78 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:31,113	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node15.unitn.it

Working with node hpc-c11-node21
then allocate other nodes:  9
node NAME: hpc-c03-node12.unitn.it
node IP: 192.168.115.105
dest IP: 192.168.115.134:6380
2022-10-04 19:22:41,549	INFO scripts.py:429 -- Using IP address 192.168.115.105 for this node.
2022-10-04 19:22:41,553	INFO resource_spec.py:212 -- Starting Ray with 235.84 GiB memory available for workers and up to 101.09 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 19:22:41,581	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c03-node12.unitn.it

done, now launching python program
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 19:23:30,943	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 19:23:30,967 seed (final): 27666000
2022-10-04 19:23:31,025	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 19:23:31,348	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 19:23:54,121	INFO trainable.py:180 -- _setup took 22.773 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 19:23:54,121	INFO trainable.py:217 -- Getting current IP.
2022-10-04 19:24:06,817	INFO trainable.py:180 -- _setup took 12.629 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 19:24:06,818	INFO trainable.py:217 -- Getting current IP.
2022-10-04 19:24:06,843 Not restoring trainer...
2022-10-04 19:24:06,844 Starting with fresh agent TF weights.
2022-10-04 19:24:06,844 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 19:29:05,369 Iter 1: steps this-iter 4000 total 4000 -> 3/5000 episodes done
2022-10-04 19:29:05,375 custom_metrics: {}
date: 2022-10-04_19-29-05
done: false
episode_len_mean: 1000.0
episode_reward_max: 297.2699660040599
episode_reward_mean: 249.32471417818837
episode_reward_min: 175.90909345725157
episodes_this_iter: 3
episodes_total: 3
experiment_id: 7355edf803da4f3aa0a75d0eb23f5668
hostname: hpc-c10-node01.unitn.it
info:
  grad_time_ms: 221359.092
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.981074571609497
      entropy_coeff: 0.02500000037252903
      kl: 0.11560539901256561
      model: {}
      policy_loss: -0.17369604110717773
      total_loss: -0.16760669648647308
      vf_explained_var: 0.8810338377952576
      vf_loss: 1.112324595451355
  load_time_ms: 3062.111
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 63421.051
  update_time_ms: 9489.642
iterations_since_restore: 1
node_ip: 192.168.115.134
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 72.29375
  ram_util_percent: 25.46953125
pid: 2774
policy_reward_max:
  agent_policy: 91.77974342337039
  planner_policy: 96.48504331404668
policy_reward_mean:
  agent_policy: 45.031572623315505
  planner_policy: 69.19842368492561
policy_reward_min:
  agent_policy: 11.09895476650544
  planner_policy: 34.93766184369637
sampler_perf:
  mean_env_wait_ms: 2.320716761749303
  mean_inference_ms: 4.29691445599538
  mean_processing_ms: 0.9127155652717504
time_since_restore: 297.75281167030334
time_this_iter_s: 297.75281167030334
time_total_s: 297.75281167030334
timestamp: 1664904545
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 19:29:05,565 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 3
=>> PBS: job killed: walltime 602 exceeded limit 600
*** Aborted at 1664904668 (unix time) try "date -d @1664904668" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xd6d) received by PID 2774 (TID 0x2b6c2f2bd3c0) from PID 3437; stack trace: ***
    @     0x2b6c2f4bb630 (unknown)
    @     0x2b6c2fec9d19 syscall
2022-10-04 19:31:08,343	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
2022-10-04 19:31:08,349	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
    @     0x2b6c77f0f479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2b6c77f0eab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2b6c77f0c0e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b6c77f0c5e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b6c76f8640b tensorflow::DirectSession::RunInternal()
    @     0x2b6c76f87829 tensorflow::DirectSession::Run()
    @     0x2b6c76f70134 tensorflow::DirectSession::Run()
    @     0x2b6c6728b8c2 tensorflow::SessionRef::Run()
    @     0x2b6c676e6f5e TF_Run_Helper()
    @     0x2b6c676e7c28 TF_SessionRun
    @     0x2b6c6728501f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b6c672850c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b6c9ab68274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b6c9ab51c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
