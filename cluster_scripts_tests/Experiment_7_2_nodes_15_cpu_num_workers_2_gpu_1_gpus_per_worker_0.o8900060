Allocate Nodes = <hpc-g04-node02 hpc-g04-node01>
set up ray cluster...


Working with node hpc-g04-node02
first allocate node - use as headnode ...
2022-10-05 12:17:16,814	INFO scripts.py:357 -- Using IP address 192.168.115.44 for this node.
2022-10-05 12:17:16,818	INFO resource_spec.py:212 -- Starting Ray with 425.54 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 12:17:17,967	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-05 12:17:18,012	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.44:3679' --redis-password='de86d538-0273-4023-ba1d-ac6e91b128a3'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='de86d538-0273-4023-ba1d-ac6e91b128a3')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-g04-node01
then allocate other nodes:  1
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.44:3679
2022-10-05 12:17:26,375	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-05 12:17:26,377	INFO resource_spec.py:212 -- Starting Ray with 529.39 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 12:17:26,399	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

done, now launching python program
Inside covid19_components.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
Inside covid19_env.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
2022-10-05 12:18:51,191	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-05 12:18:51,225 seed (final): 23051000
2022-10-05 12:18:51,288	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-05 12:18:51,562	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-05 12:19:15,070	INFO trainable.py:180 -- _setup took 23.509 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 12:19:15,071	INFO trainable.py:217 -- Getting current IP.
2022-10-05 12:19:28,793	INFO trainable.py:180 -- _setup took 13.632 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 12:19:28,794	INFO trainable.py:217 -- Getting current IP.
2022-10-05 12:19:28,823 Not restoring trainer...
2022-10-05 12:19:28,824 Starting with fresh agent TF weights.
2022-10-05 12:19:28,824 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-05 12:22:56,272 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-05 12:22:56,277 custom_metrics: {}
date: 2022-10-05_12-22-56
done: false
episode_len_mean: 1000.0
episode_reward_max: 455.5188946760083
episode_reward_mean: 310.554299038527
episode_reward_min: 154.83064827701577
episodes_this_iter: 4
episodes_total: 4
experiment_id: 536250a968b54044a888084cbf887967
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 150496.307
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 2.0305848121643066
      entropy_coeff: 0.02500000037252903
      kl: 0.11248736828565598
      model: {}
      policy_loss: -0.17626886069774628
      total_loss: -0.1605176329612732
      vf_explained_var: 0.8765523433685303
      vf_loss: 1.330317497253418
  load_time_ms: 1480.404
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 47206.847
  update_time_ms: 7058.116
iterations_since_restore: 1
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.35182481751825
  gpu_util_percent0: 0.0
  ram_util_percent: 18.615328467153287
  vram_util_percent0: 0.018997524752475244
pid: 44972
policy_reward_max:
  agent_policy: 118.5634656331842
  planner_policy: 168.42338700779845
policy_reward_mean:
  agent_policy: 53.19096055670076
  planner_policy: 97.7904568117236
policy_reward_min:
  agent_policy: -0.7293257461079774
  planner_policy: 37.3970902459972
sampler_perf:
  mean_env_wait_ms: 2.05563402723992
  mean_inference_ms: 2.62117225012143
  mean_processing_ms: 0.6245481318560081
time_since_restore: 206.53639578819275
time_this_iter_s: 206.53639578819275
time_total_s: 206.53639578819275
timestamp: 1664965376
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-05 12:22:56,469 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
2022-10-05 12:25:34,899 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-05 12:25:34,906 custom_metrics: {}
date: 2022-10-05_12-25-34
done: false
episode_len_mean: 1000.0
episode_reward_max: 196.66371726123717
episode_reward_mean: 140.02378674662347
episode_reward_min: 68.94154121761106
episodes_this_iter: 4
episodes_total: 8
experiment_id: 536250a968b54044a888084cbf887967
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 148898.032
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8288846015930176
      entropy_coeff: 0.02500000037252903
      kl: 0.12419324368238449
      model: {}
      policy_loss: -0.1881769597530365
      total_loss: -0.2045074701309204
      vf_explained_var: 0.9298189878463745
      vf_loss: 0.5878320336341858
  load_time_ms: 1253.695
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 28612.289
  update_time_ms: 3532.475
iterations_since_restore: 2
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 36.3559585492228
  gpu_util_percent0: 0.0
  ram_util_percent: 18.657512953367874
  vram_util_percent0: 0.018997524752475244
pid: 44972
policy_reward_max:
  agent_policy: 63.71051156267856
  planner_policy: 61.82081321058179
policy_reward_mean:
  agent_policy: 24.246640063574176
  planner_policy: 43.03722649232701
policy_reward_min:
  agent_policy: 6.733637685037288
  planner_policy: 26.31968161141198
sampler_perf:
  mean_env_wait_ms: 1.9634801189591364
  mean_inference_ms: 2.4315357148662207
  mean_processing_ms: 0.6086326664431456
time_since_restore: 364.9429750442505
time_this_iter_s: 158.40657925605774
time_total_s: 364.9429750442505
timestamp: 1664965534
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

-- PPO Agents -- Steps done: 8
2022-10-05 12:28:06,389 Iter 3: steps this-iter 4000 total 12000 -> 12/5000 episodes done
2022-10-05 12:28:06,396 custom_metrics: {}
date: 2022-10-05_12-28-06
done: false
episode_len_mean: 1000.0
episode_reward_max: 392.76035341741743
episode_reward_mean: 232.88023021406113
episode_reward_min: 55.11861434774805
episodes_this_iter: 4
episodes_total: 12
experiment_id: 536250a968b54044a888084cbf887967
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 145838.564
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.869686484336853
      entropy_coeff: 0.02500000037252903
      kl: 0.15208810567855835
      model: {}
      policy_loss: -0.1951763927936554
      total_loss: -0.20798993110656738
      vf_explained_var: 0.9452300071716309
      vf_loss: 0.6785727739334106
  load_time_ms: 1289.774
  num_steps_sampled: 12000
  num_steps_trained: 48000
  sample_time_ms: 22512.768
  update_time_ms: 2359.604
iterations_since_restore: 3
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 48.09619565217392
  gpu_util_percent0: 0.0
  ram_util_percent: 19.86304347826087
  vram_util_percent0: 0.018997524752475244
pid: 44972
policy_reward_max:
  agent_policy: 99.41096746864716
  planner_policy: 129.74411550587325
policy_reward_mean:
  agent_policy: 39.93415941564721
  planner_policy: 73.14359255147274
policy_reward_min:
  agent_policy: -3.4339430902405823
  planner_policy: 18.05442708239495
sampler_perf:
  mean_env_wait_ms: 1.9384016694277253
  mean_inference_ms: 2.378544932980276
  mean_processing_ms: 0.6178458756833172
time_since_restore: 516.401074886322
time_this_iter_s: 151.45809984207153
time_total_s: 516.401074886322
timestamp: 1664965686
timesteps_since_restore: 12000
timesteps_this_iter: 4000
timesteps_total: 12000
training_iteration: 3

-- PPO Agents -- Steps done: 12
=>> PBS: job killed: walltime 689 exceeded limit 600
*** Aborted at 1664965723 (unix time) try "date -d @1664965723" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xfe7c) received by PID 44972 (TID 0x2b232efbf3c0) from PID 65148; stack trace: ***
    @     0x2b232f1b8630 (unknown)
2022-10-05 12:28:43,707	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
2022-10-05 12:28:43,709	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
    @     0x2b2398f8847c tensorflow::ResourceMgr::ResourceAndName::~ResourceAndName()
    @     0x2b2398f8f417 tensorflow::ResourceMgr::Cleanup()
    @     0x2b2376c72920 _ZNSt17_Function_handlerIFvRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEZN10tensorflow13DirectSession8RunStateC4ElPKSt6vectorIPNS9_6DeviceESaISE_EEEUlS7_E_E9_M_invokeERKSt9_Any_dataS7_
    @     0x2b2376c86299 tensorflow::DirectSession::RunInternal()
    @     0x2b2376c88829 tensorflow::DirectSession::Run()
    @     0x2b2376c71134 tensorflow::DirectSession::Run()
    @     0x2b2366f8c8c2 tensorflow::SessionRef::Run()
    @     0x2b23673e7f5e TF_Run_Helper()
    @     0x2b23673e8c28 TF_SessionRun
    @     0x2b2366f8601f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b2366f860c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b239ab02274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b239aaebc98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f87a0 _PyEval_EvalFrameDefault
