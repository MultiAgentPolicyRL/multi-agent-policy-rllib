Allocate Nodes = <hpc-c10-node01 hpc-c07-node12 hpc-g04-node01 hpc-c03-node24>
set up ray cluster...


Working with node hpc-c10-node01
first allocate node - use as headnode ...
2022-10-04 18:33:54,200	INFO scripts.py:357 -- Using IP address 192.168.115.134 for this node.
2022-10-04 18:33:54,207	INFO resource_spec.py:212 -- Starting Ray with 281.45 GiB memory available for workers and up to 124.63 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:33:55,240	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 18:33:55,371	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.134:6379' --redis-password='29cc46ea-0c67-48c1-8d45-e0858a9c4d1b'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='29cc46ea-0c67-48c1-8d45-e0858a9c4d1b')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c07-node12
then allocate other nodes:  1
node NAME: hpc-c07-node12.unitn.it
node IP: 192.168.115.53
dest IP: 192.168.115.134:6379
2022-10-04 18:34:03,537	INFO scripts.py:429 -- Using IP address 192.168.115.53 for this node.
2022-10-04 18:34:03,541	INFO resource_spec.py:212 -- Starting Ray with 138.43 GiB memory available for workers and up to 59.34 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:34:03,562	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node12.unitn.it

Working with node hpc-g04-node01
then allocate other nodes:  2
node NAME: hpc-c07-node12.unitn.it
node IP: 192.168.115.53
dest IP: 192.168.115.134:6379
2022-10-04 18:34:13,478	INFO scripts.py:429 -- Using IP address 192.168.115.53 for this node.
2022-10-04 18:34:13,481	INFO resource_spec.py:212 -- Starting Ray with 138.38 GiB memory available for workers and up to 59.33 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:34:13,507	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node12.unitn.it

Working with node hpc-c03-node24
then allocate other nodes:  3
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.134:6379
2022-10-04 18:34:23,368	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-04 18:34:23,371	INFO resource_spec.py:212 -- Starting Ray with 529.3 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:34:23,397	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

done, now launching python program
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 18:36:30,616	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 18:36:30,641 seed (final): 24846000
2022-10-04 18:36:30,702	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 18:36:31,079	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 18:36:45,956	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.134: 1.000000}, {CPU: 96.000000}, {memory: 281.445312 GiB}, {object_store_memory: 85.986328 GiB}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-04 18:36:53,698	INFO trainable.py:180 -- _setup took 22.620 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:36:53,698	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:37:11,667	INFO trainable.py:180 -- _setup took 17.904 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:37:11,667	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:37:11,696 Not restoring trainer...
2022-10-04 18:37:11,697 Starting with fresh agent TF weights.
2022-10-04 18:37:11,697 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 18:41:32,889 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-04 18:41:32,895 custom_metrics: {}
date: 2022-10-04_18-41-32
done: false
episode_len_mean: 1000.0
episode_reward_max: 151.67311427853213
episode_reward_mean: 66.08526771382797
episode_reward_min: -4.710381211209924
episodes_this_iter: 4
episodes_total: 4
experiment_id: c2d02e251c7e475bb79445453c8701a6
hostname: hpc-c10-node01.unitn.it
info:
  grad_time_ms: 179869.049
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7026050090789795
      entropy_coeff: 0.02500000037252903
      kl: 0.07947292923927307
      model: {}
      policy_loss: -0.11466911435127258
      total_loss: -0.13835877180099487
      vf_explained_var: 0.7408760190010071
      vf_loss: 0.37750887870788574
  load_time_ms: 2004.067
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 70780.498
  update_time_ms: 6958.154
iterations_since_restore: 1
node_ip: 192.168.115.134
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 56.690422535211276
  ram_util_percent: 19.416901408450705
pid: 95294
policy_reward_max:
  agent_policy: 43.53987397635262
  planner_policy: 44.93802974045924
policy_reward_mean:
  agent_policy: 12.348613574688768
  planner_policy: 16.690813415073
policy_reward_min:
  agent_policy: -1.64748743164993
  planner_policy: 0.0
sampler_perf:
  mean_env_wait_ms: 2.0077014672404703
  mean_inference_ms: 3.786353335745152
  mean_processing_ms: 0.746608137905687
time_since_restore: 260.0345070362091
time_this_iter_s: 260.0345070362091
time_total_s: 260.0345070362091
timestamp: 1664901692
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 18:41:33,080 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
=>> PBS: job killed: walltime 601 exceeded limit 600
*** Aborted at 1664901832 (unix time) try "date -d @1664901832" if you are using GNU date ***
PC: @                0x0 (unknown)
2022-10-04 18:43:52,551	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
2022-10-04 18:43:52,549	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
*** SIGTERM (@0xd6d) received by PID 95294 (TID 0x2b75a1ec43c0) from PID 3437; stack trace: ***
    @     0x2b75a20c2630 (unknown)
    @     0x2b75a2ad0d19 syscall
    @     0x2b75eab16479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2b75eab15ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2b75eab130e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b75eab135e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b75e9b8d40b tensorflow::DirectSession::RunInternal()
    @     0x2b75e9b8e829 tensorflow::DirectSession::Run()
    @     0x2b75e9b77134 tensorflow::DirectSession::Run()
    @     0x2b75d9e928c2 tensorflow::SessionRef::Run()
    @     0x2b75da2edf5e TF_Run_Helper()
    @     0x2b75da2eec28 TF_SessionRun
    @     0x2b75d9e8c01f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b75d9e8c0c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b760d76f274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b760d758c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
