Allocate Nodes = <hpc-g04-node02 hpc-g04-node01 hpc-g01-node02 hpc-g02-node02>
set up ray cluster...


Working with node hpc-g04-node02
first allocate node - use as headnode ...
2022-10-05 11:53:14,801	INFO scripts.py:357 -- Using IP address 192.168.115.44 for this node.
2022-10-05 11:53:14,805	INFO resource_spec.py:212 -- Starting Ray with 402.69 GiB memory available for workers and up to 176.58 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:53:15,734	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-05 11:53:15,776	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.44:3679' --redis-password='cd3b24c8-335d-45a9-8751-851514be1b0f'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='cd3b24c8-335d-45a9-8751-851514be1b0f')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-g04-node01
then allocate other nodes:  1
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.44:3679
2022-10-05 11:53:23,769	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-05 11:53:23,772	INFO resource_spec.py:212 -- Starting Ray with 529.44 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:53:23,794	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

Working with node hpc-g01-node02
then allocate other nodes:  2
node NAME: hpc-g01-node02.unitn.it
node IP: 192.168.115.93
dest IP: 192.168.115.44:3679
2022-10-05 11:53:34,191	INFO scripts.py:429 -- Using IP address 192.168.115.93 for this node.
2022-10-05 11:53:34,194	INFO resource_spec.py:212 -- Starting Ray with 173.29 GiB memory available for workers and up to 74.28 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:53:34,194	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 55419244544 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2022-10-05 11:53:34,212	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g01-node02.unitn.it

Working with node hpc-g02-node02
then allocate other nodes:  3
node NAME: hpc-g02-node02.unitn.it
node IP: 192.168.115.10
dest IP: 192.168.115.44:3679
2022-10-05 11:53:44,264	INFO scripts.py:429 -- Using IP address 192.168.115.10 for this node.
2022-10-05 11:53:44,267	INFO resource_spec.py:212 -- Starting Ray with 218.46 GiB memory available for workers and up to 93.64 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 11:53:44,267	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 68429537280 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2022-10-05 11:53:44,285	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g02-node02.unitn.it

done, now launching python program
Inside covid19_components.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
Inside covid19_env.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
2022-10-05 11:54:52,072	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-05 11:54:52,100 seed (final): 21612000
2022-10-05 11:54:52,162	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-05 11:54:52,401	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-05 11:55:06,260	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.44: 1.000000}, {object_store_memory: 121.826172 GiB}, {CPU: 72.000000}, {memory: 402.685547 GiB}, {GPU: 1.000000}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-05 11:55:16,849	INFO trainable.py:180 -- _setup took 24.450 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 11:55:16,850	INFO trainable.py:217 -- Getting current IP.
2022-10-05 11:55:32,006	INFO trainable.py:180 -- _setup took 15.060 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 11:55:32,006	INFO trainable.py:217 -- Getting current IP.
2022-10-05 11:55:32,037 Not restoring trainer...
2022-10-05 11:55:32,038 Starting with fresh agent TF weights.
2022-10-05 11:55:32,038 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-05 11:58:47,978 Iter 1: steps this-iter 4000 total 4000 -> 3/5000 episodes done
2022-10-05 11:58:47,984 custom_metrics: {}
date: 2022-10-05_11-58-47
done: false
episode_len_mean: 1000.0
episode_reward_max: 399.2333465262067
episode_reward_mean: 378.80880174510565
episode_reward_min: 361.73223450861883
episodes_this_iter: 3
episodes_total: 3
experiment_id: 3a580fe47a0e47b7bb2c99178a68446a
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 147293.602
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 2.0937697887420654
      entropy_coeff: 0.02500000037252903
      kl: 0.11736254394054413
      model: {}
      policy_loss: -0.18127723038196564
      total_loss: -0.1696816235780716
      vf_explained_var: 0.8941650390625
      vf_loss: 1.2787970304489136
  load_time_ms: 2050.177
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 37849.361
  update_time_ms: 7346.583
iterations_since_restore: 1
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 55.323954372623575
  gpu_util_percent0: 0.0
  ram_util_percent: 22.79239543726236
  vram_util_percent0: 0.018997524752475244
pid: 37469
policy_reward_max:
  agent_policy: 115.48741754867233
  planner_policy: 142.900474849243
policy_reward_mean:
  agent_policy: 64.08057930912223
  planner_policy: 122.48648450861644
policy_reward_min:
  agent_policy: 15.510169831610593
  planner_policy: 104.7509236121809
sampler_perf:
  mean_env_wait_ms: 2.245692244565092
  mean_inference_ms: 2.6881642977971083
  mean_processing_ms: 0.6567353902815084
time_since_restore: 194.97016596794128
time_this_iter_s: 194.97016596794128
time_total_s: 194.97016596794128
timestamp: 1664963927
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-05 11:58:48,178 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 3
*** Aborted at 1664964076 (unix time) try "date -d @1664964076" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xfe7c) received by PID 37469 (TID 0x2b85157bf3c0) from PID 65148; stack trace: ***
    @     0x2b85159b8630 (unknown)
    @     0x2b85163c6bf9 syscall
    @     0x2b855e410479 nsync::nsync_mu_semaphore_p_with_deadline()
2022-10-05 12:01:16,305	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
2022-10-05 12:01:16,313	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
    @     0x2b855e40fab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2b855e40d0e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b855e40d5e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b855d48740b tensorflow::DirectSession::RunInternal()
    @     0x2b855d488829 tensorflow::DirectSession::Run()
    @     0x2b855d471134 tensorflow::DirectSession::Run()
    @     0x2b854d78c8c2 tensorflow::SessionRef::Run()
    @     0x2b854dbe7f5e TF_Run_Helper()
    @     0x2b854dbe8c28 TF_SessionRun
    @     0x2b854d78601f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b854d7860c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b8581302274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b85812ebc98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
