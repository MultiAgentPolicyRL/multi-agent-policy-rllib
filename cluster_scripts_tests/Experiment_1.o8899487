Allocate Nodes = <hpc-c11-node11 hpc-c11-node13 hpc-c11-node15 hpc-c11-node07>
set up ray cluster...


Working with node hpc-c11-node11
first allocate node - use as headnode ...
2022-10-04 18:05:01,985	INFO scripts.py:357 -- Using IP address 192.168.115.78 for this node.
2022-10-04 18:05:01,996	INFO resource_spec.py:212 -- Starting Ray with 645.02 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:05:02,948	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 18:05:02,985	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.78:6379' --redis-password='3dd2b13b-73c0-474f-8cdf-11d2cc70e0f4'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='3dd2b13b-73c0-474f-8cdf-11d2cc70e0f4')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c11-node13
then allocate other nodes:  1
node NAME: hpc-c11-node13.unitn.it
node IP: 192.168.115.83
dest IP: 192.168.115.78:6379
2022-10-04 18:05:14,555	INFO scripts.py:429 -- Using IP address 192.168.115.83 for this node.
2022-10-04 18:05:14,567	INFO resource_spec.py:212 -- Starting Ray with 754.49 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:05:14,598	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node13.unitn.it

Working with node hpc-c11-node15
then allocate other nodes:  2
node NAME: hpc-c11-node15.unitn.it
node IP: 192.168.115.81
dest IP: 192.168.115.78:6379
2022-10-04 18:05:24,275	INFO scripts.py:429 -- Using IP address 192.168.115.81 for this node.
2022-10-04 18:05:24,286	INFO resource_spec.py:212 -- Starting Ray with 775.39 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:05:24,315	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node15.unitn.it

Working with node hpc-c11-node07
then allocate other nodes:  3
node NAME: hpc-c11-node07.unitn.it
node IP: 192.168.115.43
dest IP: 192.168.115.78:6379
2022-10-04 18:05:35,787	INFO scripts.py:429 -- Using IP address 192.168.115.43 for this node.
2022-10-04 18:05:35,791	INFO resource_spec.py:212 -- Starting Ray with 318.41 GiB memory available for workers and up to 136.47 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:05:35,814	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

done, now launching python program
exiting hpc-c11-node07.unitn.it
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 18:07:47,182	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 18:07:47,212 seed (final): 23123000
2022-10-04 18:07:47,276	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 18:07:47,528	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 18:08:09,706	INFO trainable.py:180 -- _setup took 22.179 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:08:09,706	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:08:24,877	INFO trainable.py:180 -- _setup took 15.045 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:08:24,877	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:08:25,006 Not restoring trainer...
2022-10-04 18:08:25,007 Starting with fresh agent TF weights.
2022-10-04 18:08:25,007 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 18:12:34,247 Iter 1: steps this-iter 4000 total 4000 -> 3/5000 episodes done
2022-10-04 18:12:34,253 custom_metrics: {}
date: 2022-10-04_18-12-34
done: false
episode_len_mean: 1000.0
episode_reward_max: 113.36279821179885
episode_reward_mean: 89.55591831378261
episode_reward_min: 53.358569707872505
episodes_this_iter: 3
episodes_total: 3
experiment_id: 5eae0b8fb554427592e01cd491be3db5
hostname: hpc-c11-node11.unitn.it
info:
  grad_time_ms: 138235.181
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8561993837356567
      entropy_coeff: 0.02500000037252903
      kl: 0.10056304186582565
      model: {}
      policy_loss: -0.1428135633468628
      total_loss: -0.15646737813949585
      vf_explained_var: 0.8597983121871948
      vf_loss: 0.6550238728523254
  load_time_ms: 1542.35
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 100387.507
  update_time_ms: 7676.504
iterations_since_restore: 1
node_ip: 192.168.115.78
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 65.66862170087977
  ram_util_percent: 16.265982404692085
pid: 97010
policy_reward_max:
  agent_policy: 35.22673319790444
  planner_policy: 28.089037446327545
policy_reward_mean:
  agent_policy: 16.85859972196374
  planner_policy: 22.121519425927783
policy_reward_min:
  agent_policy: -3.0173299736140633
  planner_policy: 12.610546874531376
sampler_perf:
  mean_env_wait_ms: 2.12612358528907
  mean_inference_ms: 2.9291716430608843
  mean_processing_ms: 0.6691256767856304
time_since_restore: 248.23980498313904
time_this_iter_s: 248.23980498313904
time_total_s: 248.23980498313904
timestamp: 1664899954
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 18:12:34,431 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 3
2022-10-04 18:15:05,765 Iter 2: steps this-iter 4000 total 8000 -> 7/5000 episodes done
2022-10-04 18:15:05,770 custom_metrics: {}
date: 2022-10-04_18-15-05
done: false
episode_len_mean: 1000.0
episode_reward_max: 246.0921990703152
episode_reward_mean: 131.46100055080004
episode_reward_min: 53.358569707872505
episodes_this_iter: 4
episodes_total: 7
experiment_id: 5eae0b8fb554427592e01cd491be3db5
hostname: hpc-c11-node11.unitn.it
info:
  grad_time_ms: 138391.811
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8947548866271973
      entropy_coeff: 0.02500000037252903
      kl: 0.1300148367881775
      model: {}
      policy_loss: -0.19085045158863068
      total_loss: -0.2025737166404724
      vf_explained_var: 0.9404070377349854
      vf_loss: 0.7129128575325012
  load_time_ms: 1283.789
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 56031.222
  update_time_ms: 3842.097
iterations_since_restore: 2
node_ip: 192.168.115.78
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 67.83796791443851
  ram_util_percent: 16.386096256684493
pid: 97010
policy_reward_max:
  agent_policy: 95.71667158125994
  planner_policy: 75.97162939125114
policy_reward_mean:
  agent_policy: 24.730729378585686
  planner_policy: 32.53808303645703
policy_reward_min:
  agent_policy: -7.030390605690417
  planner_policy: 12.610546874531376
sampler_perf:
  mean_env_wait_ms: 2.1114126495851795
  mean_inference_ms: 2.896341677906505
  mean_processing_ms: 0.6711926359715022
time_since_restore: 399.55219769477844
time_this_iter_s: 151.3123927116394
time_total_s: 399.55219769477844
timestamp: 1664900105
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

2022-10-04 18:15:05,976 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000008000
-- PPO Agents -- Steps done: 7
=>> PBS: job killed: walltime 688 exceeded limit 600
*** Aborted at 1664900187 (unix time) try "date -d @1664900187" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xde6) received by PID 97010 (TID 0x2aef62ae23c0) from PID 3558; stack trace: ***
    @     0x2aef62ce0630 (unknown)
    @     0x2aef636eee29 syscall
2022-10-04 18:16:27,710	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
2022-10-04 18:16:27,715	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
    @     0x2aefab734479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2aefab733ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2aefab7310e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2aefab7315e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2aefaa7ab40b tensorflow::DirectSession::RunInternal()
    @     0x2aefaa7ac829 tensorflow::DirectSession::Run()
    @     0x2aefaa795134 tensorflow::DirectSession::Run()
    @     0x2aef9aab08c2 tensorflow::SessionRef::Run()
    @     0x2aef9af0bf5e TF_Run_Helper()
    @     0x2aef9af0cc28 TF_SessionRun
    @     0x2aef9aaaa01f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2aef9aaaa0c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2aefce38d274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2aefce376c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
