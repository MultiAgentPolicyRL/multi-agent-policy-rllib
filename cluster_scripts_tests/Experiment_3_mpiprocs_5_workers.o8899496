Allocate Nodes = <hpc-c11-node08 hpc-c03-node17 hpc-c11-node10>
set up ray cluster...


Working with node hpc-c11-node08
first allocate node - use as headnode ...
2022-10-04 18:34:57,240	INFO scripts.py:357 -- Using IP address 192.168.115.142 for this node.
2022-10-04 18:34:57,246	INFO resource_spec.py:212 -- Starting Ray with 334.62 GiB memory available for workers and up to 147.42 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:34:58,506	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 18:34:58,543	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.142:6379' --redis-password='a472ff4e-4d75-494f-ba84-3d05182e5d03'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='a472ff4e-4d75-494f-ba84-3d05182e5d03')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c03-node17
then allocate other nodes:  1
node NAME: hpc-c11-node08.unitn.it
node IP: 192.168.115.142
dest IP: 192.168.115.142:6379

Working with node hpc-c11-node10
then allocate other nodes:  2
node NAME: hpc-c03-node17.unitn.it
node IP: 192.168.115.8
dest IP: 192.168.115.142:6379
2022-10-04 18:35:20,342	INFO scripts.py:429 -- Using IP address 192.168.115.8 for this node.
2022-10-04 18:35:20,345	INFO resource_spec.py:212 -- Starting Ray with 261.96 GiB memory available for workers and up to 112.28 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:35:20,383	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c03-node17.unitn.it

done, now launching python program
2022-10-04 18:37:07,469	INFO scripts.py:429 -- Using IP address 192.168.115.142 for this node.
2022-10-04 18:37:07,474	INFO resource_spec.py:212 -- Starting Ray with 341.75 GiB memory available for workers and up to 146.48 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:37:07,511	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node08.unitn.it
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 18:37:45,464	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 18:37:45,489 seed (final): 24921000
2022-10-04 18:37:45,550	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 18:37:45,843	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 18:37:59,238	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.142: 1.000000}, {CPU: 96.000000}, {memory: 334.619141 GiB}, {object_store_memory: 101.708984 GiB}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-04 18:38:06,354	INFO trainable.py:180 -- _setup took 20.513 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:38:06,355	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:38:19,394	INFO trainable.py:180 -- _setup took 12.931 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:38:19,394	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:38:19,398 Not restoring trainer...
2022-10-04 18:38:19,418 Starting with fresh agent TF weights.
2022-10-04 18:38:19,419 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 18:41:35,986 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-04 18:41:35,991 custom_metrics: {}
date: 2022-10-04_18-41-35
done: false
episode_len_mean: 1000.0
episode_reward_max: 156.9692467586719
episode_reward_mean: 105.58375930781887
episode_reward_min: 79.38443078615707
episodes_this_iter: 4
episodes_total: 4
experiment_id: 8c195b405f424146b340c3a10ecebf19
hostname: hpc-c11-node08.unitn.it
info:
  grad_time_ms: 120871.086
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7508679628372192
      entropy_coeff: 0.02500000037252903
      kl: 0.10905112326145172
      model: {}
      policy_loss: -0.1383417546749115
      total_loss: -0.13957028090953827
      vf_explained_var: 0.8526178598403931
      vf_loss: 0.850863516330719
  load_time_ms: 1050.412
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 67180.624
  update_time_ms: 6408.732
iterations_since_restore: 1
node_ip: 192.168.115.142
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 39.824909747292416
  ram_util_percent: 3.680505415162455
pid: 16919
policy_reward_max:
  agent_policy: 58.04186076624346
  planner_policy: 34.35295720880805
policy_reward_mean:
  agent_policy: 20.96661134111701
  planner_policy: 21.71731394334997
policy_reward_min:
  agent_policy: -0.06277135988644611
  planner_policy: 15.717768438175565
sampler_perf:
  mean_env_wait_ms: 1.938610598780047
  mean_inference_ms: 2.690848560704999
  mean_processing_ms: 0.5762622929524923
time_since_restore: 195.75679278373718
time_this_iter_s: 195.75679278373718
time_total_s: 195.75679278373718
timestamp: 1664901695
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 18:41:36,153 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
2022-10-04 18:43:46,450 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-04 18:43:46,455 custom_metrics: {}
date: 2022-10-04_18-43-46
done: false
episode_len_mean: 1000.0
episode_reward_max: 156.9692467586719
episode_reward_mean: 101.8336507425434
episode_reward_min: 79.38443078615707
episodes_this_iter: 4
episodes_total: 8
experiment_id: 8c195b405f424146b340c3a10ecebf19
hostname: hpc-c11-node08.unitn.it
info:
  grad_time_ms: 120097.875
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.779289960861206
      entropy_coeff: 0.02500000037252903
      kl: 0.12427574396133423
      model: {}
      policy_loss: -0.17538078129291534
      total_loss: -0.19445864856243134
      vf_explained_var: 0.9227468371391296
      vf_loss: 0.5080879330635071
  load_time_ms: 986.687
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 38573.413
  update_time_ms: 3208.447
iterations_since_restore: 2
node_ip: 192.168.115.142
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 41.30714285714286
  ram_util_percent: 3.8761904761904753
pid: 16919
policy_reward_max:
  agent_policy: 74.06064808178569
  planner_policy: 34.35295720880805
policy_reward_mean:
  agent_policy: 19.92392889318181
  planner_policy: 22.137935169815375
policy_reward_min:
  agent_policy: -6.5145709006871995
  planner_policy: 15.717768438175565
sampler_perf:
  mean_env_wait_ms: 1.9066618275389464
  mean_inference_ms: 2.60769964137808
  mean_processing_ms: 0.5736830257993918
time_since_restore: 326.0326838493347
time_this_iter_s: 130.27589106559753
time_total_s: 326.0326838493347
timestamp: 1664901826
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

-- PPO Agents -- Steps done: 8
=>> PBS: job killed: walltime 604 exceeded limit 600
*** Aborted at 1664901898 (unix time) try "date -d @1664901898" if you are using GNU date ***
PC: @                0x0 (unknown)
2022-10-04 18:44:58,703	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
2022-10-04 18:44:58,704	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
*** SIGTERM (@0xdcb) received by PID 16919 (TID 0x2af1f96c03c0) from PID 3531; stack trace: ***
    @     0x2af1f98be630 (unknown)
    @     0x2af1fa2ccd19 syscall
    @     0x2af242312479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2af242311ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2af24230f0e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2af24230f5e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2af24138940b tensorflow::DirectSession::RunInternal()
    @     0x2af24138a829 tensorflow::DirectSession::Run()
    @     0x2af241373134 tensorflow::DirectSession::Run()
    @     0x2af23168e8c2 tensorflow::SessionRef::Run()
    @     0x2af231ae9f5e TF_Run_Helper()
    @     0x2af231aeac28 TF_SessionRun
    @     0x2af23168801f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2af2316880c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2af264f6b274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2af264f54c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
