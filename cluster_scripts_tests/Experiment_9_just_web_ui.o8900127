HEAD NODE:  thishost
Allocate Nodes = <hpc-g04-node02 hpc-g04-node01>
set up ray cluster...


Working with node hpc-g04-node02
first allocate node - use as headnode ...
2022-10-05 13:59:49,753	INFO scripts.py:357 -- Using IP address 192.168.115.44 for this node.
2022-10-05 13:59:49,757	INFO resource_spec.py:212 -- Starting Ray with 393.65 GiB memory available for workers and up to 172.7 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 13:59:50,696	INFO services.py:1170 -- View the Ray dashboard at [1m[32m192.168.115.44:8265[39m[22m
2022-10-05 13:59:50,732	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.44:3679' --redis-password='9e9e15ef-c7fd-4518-bf8c-9f8d7303f540'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='9e9e15ef-c7fd-4518-bf8c-9f8d7303f540')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-g04-node01
then allocate other nodes:  1
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.44:3679
2022-10-05 13:59:58,212	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-05 13:59:58,215	INFO resource_spec.py:212 -- Starting Ray with 529.44 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 13:59:58,237	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

done, now launching python program
Inside covid19_components.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
Inside covid19_env.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
2022-10-05 14:01:25,644	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-05 14:01:25,677 seed (final): 29205000
2022-10-05 14:01:25,741	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-05 14:01:25,993	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-05 14:01:41,262	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.44: 1.000000}, {object_store_memory: 119.140625 GiB}, {CPU: 72.000000}, {memory: 393.652344 GiB}, {GPU: 1.000000}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-05 14:01:48,924	INFO trainable.py:180 -- _setup took 22.933 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 14:01:48,924	INFO trainable.py:217 -- Getting current IP.
2022-10-05 14:02:02,826	INFO trainable.py:180 -- _setup took 13.829 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 14:02:02,827	INFO trainable.py:217 -- Getting current IP.
2022-10-05 14:02:02,854 Not restoring trainer...
2022-10-05 14:02:02,854 Starting with fresh agent TF weights.
2022-10-05 14:02:02,854 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
*** Aborted at 1664971439 (unix time) try "date -d @1664971439" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xfe7c) received by PID 4498 (TID 0x2b70bfe243c0) from PID 65148; stack trace: ***
    @     0x2b70c001d630 (unknown)
    @     0x2b70c0a2bbf9 syscall
    @     0x2b7108a75479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2b7108a74ab9 nsync::nsync_sem_wait_with_cancel_()
2022-10-05 14:03:59,737	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
2022-10-05 14:03:59,739	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
    @     0x2b7108a720e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b7108a725e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b7107aec40b tensorflow::DirectSession::RunInternal()
    @     0x2b7107aed829 tensorflow::DirectSession::Run()
    @     0x2b7107ad6134 tensorflow::DirectSession::Run()
    @     0x2b70f7df18c2 tensorflow::SessionRef::Run()
    @     0x2b70f824cf5e TF_Run_Helper()
    @     0x2b70f824dc28 TF_SessionRun
    @     0x2b70f7deb01f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b70f7deb0c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b712b967274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b712b950c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
