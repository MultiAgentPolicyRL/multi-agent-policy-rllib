Allocate Nodes = <hpc-c11-node18 hpc-c11-node12 hpc-c11-node16 hpc-c11-node14 hpc-c11-node13 hpc-c04-node01 hpc-c05-node01 hpc-c05-node04 hpc-c07-node03 hpc-c07-node02 hpc-c07-node07 hpc-c07-node11 hpc-c07-node08 hpc-c07-node01 hpc-c07-node13 hpc-c11-node15>
set up ray cluster...


Working with node hpc-c11-node18
first allocate node - use as headnode ...
2022-10-04 20:29:07,056	INFO scripts.py:357 -- Using IP address 192.168.115.145 for this node.
2022-10-04 20:29:07,061	INFO resource_spec.py:212 -- Starting Ray with 279.44 GiB memory available for workers and up to 123.76 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:29:08,119	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 20:29:08,146	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.145:3679' --redis-password='a2753127-de58-444d-babf-116834e8751b'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='a2753127-de58-444d-babf-116834e8751b')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c11-node12
then allocate other nodes:  1
node NAME: hpc-c11-node12.unitn.it
node IP: 192.168.115.66
dest IP: 192.168.115.145:3679
2022-10-04 20:29:21,982	INFO scripts.py:429 -- Using IP address 192.168.115.66 for this node.
2022-10-04 20:29:21,990	INFO resource_spec.py:212 -- Starting Ray with 441.36 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:29:22,018	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node12.unitn.it

Working with node hpc-c11-node16
then allocate other nodes:  2
node NAME: hpc-c11-node16.unitn.it
node IP: 192.168.115.95
dest IP: 192.168.115.145:3679
2022-10-04 20:29:31,038	INFO scripts.py:429 -- Using IP address 192.168.115.95 for this node.
2022-10-04 20:29:31,045	INFO resource_spec.py:212 -- Starting Ray with 637.5 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:29:31,072	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node16.unitn.it

Working with node hpc-c11-node14
then allocate other nodes:  3
node NAME: hpc-c11-node14.unitn.it
node IP: 192.168.115.97
dest IP: 192.168.115.145:3679
2022-10-04 20:29:41,978	INFO scripts.py:429 -- Using IP address 192.168.115.97 for this node.
2022-10-04 20:29:41,993	INFO resource_spec.py:212 -- Starting Ray with 665.87 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:29:42,038	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node14.unitn.it

Working with node hpc-c11-node13
then allocate other nodes:  4
node NAME: hpc-c11-node14.unitn.it
node IP: 192.168.115.97
dest IP: 192.168.115.145:3679
2022-10-04 20:29:51,519	INFO scripts.py:429 -- Using IP address 192.168.115.97 for this node.
2022-10-04 20:29:51,531	INFO resource_spec.py:212 -- Starting Ray with 673.1 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:29:51,556	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node14.unitn.it

Working with node hpc-c04-node01
then allocate other nodes:  5
node NAME: hpc-c11-node13.unitn.it
node IP: 192.168.115.83
dest IP: 192.168.115.145:3679
2022-10-04 20:30:01,338	INFO scripts.py:429 -- Using IP address 192.168.115.83 for this node.
2022-10-04 20:30:01,350	INFO resource_spec.py:212 -- Starting Ray with 736.52 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:30:01,381	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node13.unitn.it

Working with node hpc-c05-node01
then allocate other nodes:  6
node NAME: hpc-c11-node13.unitn.it
node IP: 192.168.115.83
dest IP: 192.168.115.145:3679
2022-10-04 20:30:10,855	INFO scripts.py:429 -- Using IP address 192.168.115.83 for this node.
2022-10-04 20:30:10,870	INFO resource_spec.py:212 -- Starting Ray with 742.29 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:30:10,898	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node13.unitn.it

Working with node hpc-c05-node04
then allocate other nodes:  7
node NAME: hpc-c11-node13.unitn.it
node IP: 192.168.115.83
dest IP: 192.168.115.145:3679
2022-10-04 20:30:20,599	INFO scripts.py:429 -- Using IP address 192.168.115.83 for this node.
2022-10-04 20:30:20,608	INFO resource_spec.py:212 -- Starting Ray with 743.7 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:30:20,632	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c07-node03
then allocate other nodes:  8
exiting hpc-c11-node13.unitn.it
node NAME: hpc-c11-node13.unitn.it
node IP: 192.168.115.83
dest IP: 192.168.115.145:3679

Working with node hpc-c07-node02
then allocate other nodes:  9
2022-10-04 20:30:38,690	INFO scripts.py:429 -- Using IP address 192.168.115.83 for this node.
2022-10-04 20:30:38,699	INFO resource_spec.py:212 -- Starting Ray with 754.1 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:30:38,722	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
node NAME: hpc-c04-node01.unitn.it
node IP: 192.168.115.4
dest IP: 192.168.115.145:3679
2022-10-04 20:30:43,922	INFO scripts.py:429 -- Using IP address 192.168.115.4 for this node.
2022-10-04 20:30:43,943	INFO resource_spec.py:212 -- Starting Ray with 548.63 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:30:43,974	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c04-node01.unitn.it

Working with node hpc-c07-node07
then allocate other nodes:  10
node NAME: hpc-c05-node01.unitn.it
node IP: 192.168.115.16
dest IP: 192.168.115.145:3679
2022-10-04 20:30:49,538	INFO scripts.py:429 -- Using IP address 192.168.115.16 for this node.
2022-10-04 20:30:49,542	INFO resource_spec.py:212 -- Starting Ray with 478.81 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:30:49,571	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c05-node01.unitn.it
exiting hpc-c11-node13.unitn.it

Working with node hpc-c07-node11
then allocate other nodes:  11
node NAME: hpc-c05-node04.unitn.it
node IP: 192.168.115.25
dest IP: 192.168.115.145:3679
2022-10-04 20:31:04,875	INFO scripts.py:429 -- Using IP address 192.168.115.25 for this node.
2022-10-04 20:31:04,879	INFO resource_spec.py:212 -- Starting Ray with 478.81 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:31:04,914	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c07-node08
then allocate other nodes:  12
node NAME: hpc-c07-node03.unitn.it
node IP: 192.168.115.29
dest IP: 192.168.115.145:3679
exiting hpc-c05-node04.unitn.it

Working with node hpc-c07-node01
then allocate other nodes:  13
node NAME: hpc-c07-node02.unitn.it
node IP: 192.168.115.28
dest IP: 192.168.115.145:3679
2022-10-04 20:31:21,594	INFO scripts.py:429 -- Using IP address 192.168.115.28 for this node.
2022-10-04 20:31:21,598	INFO resource_spec.py:212 -- Starting Ray with 92.92 GiB memory available for workers and up to 39.84 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:31:21,621	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node02.unitn.it
2022-10-04 20:31:24,175	INFO scripts.py:429 -- Using IP address 192.168.115.29 for this node.
2022-10-04 20:31:24,179	INFO resource_spec.py:212 -- Starting Ray with 116.36 GiB memory available for workers and up to 49.88 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:31:24,478	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node03.unitn.it

Working with node hpc-c07-node13
then allocate other nodes:  14
node NAME: hpc-c07-node07.unitn.it
node IP: 192.168.115.59
dest IP: 192.168.115.145:3679
2022-10-04 20:31:30,574	INFO scripts.py:429 -- Using IP address 192.168.115.59 for this node.
2022-10-04 20:31:30,577	INFO resource_spec.py:212 -- Starting Ray with 114.94 GiB memory available for workers and up to 49.27 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:31:30,603	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node07.unitn.it

Working with node hpc-c11-node15
then allocate other nodes:  15
node NAME: hpc-c07-node11.unitn.it
node IP: 192.168.115.52
dest IP: 192.168.115.145:3679

done, now launching python program
2022-10-04 20:31:50,782	INFO scripts.py:429 -- Using IP address 192.168.115.52 for this node.
2022-10-04 20:31:50,794	INFO resource_spec.py:212 -- Starting Ray with 95.7 GiB memory available for workers and up to 41.03 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:31:50,918	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node11.unitn.it
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 20:32:29,432	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 20:32:29,464 seed (final): 31805000
2022-10-04 20:32:29,528	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 20:32:29,861	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 20:32:53,029	INFO trainable.py:180 -- _setup took 23.169 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 20:32:53,029	INFO trainable.py:217 -- Getting current IP.
2022-10-04 20:33:08,402	INFO trainable.py:180 -- _setup took 15.286 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 20:33:08,403	INFO trainable.py:217 -- Getting current IP.
2022-10-04 20:33:08,426 Not restoring trainer...
2022-10-04 20:33:08,427 Starting with fresh agent TF weights.
2022-10-04 20:33:08,427 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 20:36:54,573 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-04 20:36:54,579 custom_metrics: {}
date: 2022-10-04_20-36-54
done: false
episode_len_mean: 1000.0
episode_reward_max: 278.4077333251957
episode_reward_mean: 200.8539858943883
episode_reward_min: 92.01560670769882
episodes_this_iter: 4
episodes_total: 4
experiment_id: 18dcda6d18274d81af8b36b596f10f42
hostname: hpc-c11-node18.unitn.it
info:
  grad_time_ms: 147746.768
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9818761348724365
      entropy_coeff: 0.02500000037252903
      kl: 0.11453960835933685
      model: {}
      policy_loss: -0.17171114683151245
      total_loss: -0.1727922558784485
      vf_explained_var: 0.8765140175819397
      vf_loss: 0.9693169593811035
  load_time_ms: 962.662
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 69794.301
  update_time_ms: 6322.434
iterations_since_restore: 1
node_ip: 192.168.115.145
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 71.1365625
  ram_util_percent: 19.369374999999998
pid: 47028
policy_reward_max:
  agent_policy: 115.59649768398954
  planner_policy: 75.19071744350114
policy_reward_mean:
  agent_policy: 37.52095110798597
  planner_policy: 50.77018146244373
policy_reward_min:
  agent_policy: -3.8494606737927812
  planner_policy: 17.38926028248467
sampler_perf:
  mean_env_wait_ms: 2.2807175489975657
  mean_inference_ms: 3.3242962230508892
  mean_processing_ms: 0.6945697978876163
time_since_restore: 225.253000497818
time_this_iter_s: 225.253000497818
time_total_s: 225.253000497818
timestamp: 1664908614
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 20:36:54,777 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
=>> PBS: job killed: walltime 601 exceeded limit 600
*** Aborted at 1664908745 (unix time) try "date -d @1664908745" if you are using GNU date ***
PC: @                0x0 (unknown)
2022-10-04 20:39:05,116	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
*** SIGTERM (@0xd8c) received by PID 47028 (TID 0x2b1d71d653c0) from PID 3468; stack trace: ***
    @     0x2b1d71f63630 (unknown)
    @     0x2b1d72971e29 syscall
2022-10-04 20:39:05,139	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
    @     0x2b1da55ae479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2b1da55adab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2b1da55ab0e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b1da55ab5e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b1da462540b tensorflow::DirectSession::RunInternal()
    @     0x2b1da4626829 tensorflow::DirectSession::Run()
    @     0x2b1da460f134 tensorflow::DirectSession::Run()
    @     0x2b1d9492a8c2 tensorflow::SessionRef::Run()
    @     0x2b1d94d85f5e TF_Run_Helper()
    @     0x2b1d94d86c28 TF_SessionRun
    @     0x2b1d9492401f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b1d949240c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b1dc8207274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b1dc81f0c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
