HEAD NODE:  thishost
Allocate Nodes = <hpc-g04-node02 hpc-g04-node01>
set up ray cluster...


Working with node hpc-g04-node02
first allocate node - use as headnode ...
2022-10-05 13:24:51,339	INFO scripts.py:357 -- Using IP address 192.168.115.44 for this node.
2022-10-05 13:24:51,343	INFO resource_spec.py:212 -- Starting Ray with 406.98 GiB memory available for workers and up to 178.43 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 13:24:52,257	INFO services.py:1170 -- View the Ray dashboard at [1m[32m192.168.115.44:8265[39m[22m
2022-10-05 13:24:52,295	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.44:3679' --redis-password='8e6e2da5-cb12-4f22-81c9-a9d8648bdede'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='8e6e2da5-cb12-4f22-81c9-a9d8648bdede')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-g04-node01
then allocate other nodes:  1
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.44:3679
2022-10-05 13:24:59,736	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-05 13:24:59,739	INFO resource_spec.py:212 -- Starting Ray with 529.44 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 13:24:59,761	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

done, now launching python program
Inside covid19_components.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
Inside covid19_env.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
2022-10-05 13:26:27,771	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-05 13:26:27,802 seed (final): 27107000
2022-10-05 13:26:27,867	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-05 13:26:28,125	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-05 13:26:54,829	INFO trainable.py:180 -- _setup took 26.706 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 13:26:54,829	INFO trainable.py:217 -- Getting current IP.
2022-10-05 13:27:11,709	INFO trainable.py:180 -- _setup took 16.800 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 13:27:11,709	INFO trainable.py:217 -- Getting current IP.
2022-10-05 13:27:11,733 Not restoring trainer...
2022-10-05 13:27:11,733 Starting with fresh agent TF weights.
2022-10-05 13:27:11,733 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-05 13:30:28,288 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-05 13:30:28,295 custom_metrics: {}
date: 2022-10-05_13-30-28
done: false
episode_len_mean: 1000.0
episode_reward_max: 233.63250389664296
episode_reward_mean: 157.45697285059384
episode_reward_min: 117.29531653240075
episodes_this_iter: 4
episodes_total: 4
experiment_id: 673d17aefbcd4b8d8c8aaa1a78122d9d
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 148659.413
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9611371755599976
      entropy_coeff: 0.02500000037252903
      kl: 0.10917260497808456
      model: {}
      policy_loss: -0.16277392208576202
      total_loss: -0.170753613114357
      vf_explained_var: 0.8813783526420593
      vf_loss: 0.8209746479988098
  load_time_ms: 1563.973
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 36446.406
  update_time_ms: 8172.776
iterations_since_restore: 1
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 38.037593984962406
  gpu_util_percent0: 0.0
  ram_util_percent: 19.57443609022556
  vram_util_percent0: 0.018997524752475247
pid: 63313
policy_reward_max:
  agent_policy: 66.41743294864692
  planner_policy: 67.03411155329209
policy_reward_mean:
  agent_policy: 28.469753738081693
  planner_policy: 43.577957898267506
policy_reward_min:
  agent_policy: 8.690967656571768
  planner_policy: 29.212731086268363
sampler_perf:
  mean_env_wait_ms: 2.040026606112227
  mean_inference_ms: 2.6737416999927466
  mean_processing_ms: 0.6341705436649351
time_since_restore: 195.1376757621765
time_this_iter_s: 195.1376757621765
time_total_s: 195.1376757621765
timestamp: 1664969428
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-05 13:30:28,469 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
2022-10-05 13:33:00,547 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-05 13:33:00,553 custom_metrics: {}
date: 2022-10-05_13-33-00
done: false
episode_len_mean: 1000.0
episode_reward_max: 265.58069680856136
episode_reward_mean: 148.94866345491147
episode_reward_min: 35.64539471862019
episodes_this_iter: 4
episodes_total: 8
experiment_id: 673d17aefbcd4b8d8c8aaa1a78122d9d
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 144456.691
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7760906219482422
      entropy_coeff: 0.02500000037252903
      kl: 0.1175638735294342
      model: {}
      policy_loss: -0.1699722409248352
      total_loss: -0.1915932595729828
      vf_explained_var: 0.9270764589309692
      vf_loss: 0.45562493801116943
  load_time_ms: 1359.854
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 23525.996
  update_time_ms: 4090.141
iterations_since_restore: 2
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.6968085106383
  gpu_util_percent0: 0.0
  ram_util_percent: 18.73351063829787
  vram_util_percent0: 0.018997524752475244
pid: 63313
policy_reward_max:
  agent_policy: 94.6582151557487
  planner_policy: 85.45478830541938
policy_reward_mean:
  agent_policy: 27.088992982233503
  planner_policy: 40.592691525977074
policy_reward_min:
  agent_policy: 2.4308942184177296
  planner_policy: 11.044518723164138
sampler_perf:
  mean_env_wait_ms: 2.013927816070812
  mean_inference_ms: 2.611943437006519
  mean_processing_ms: 0.6364421317694762
time_since_restore: 347.19752192497253
time_this_iter_s: 152.05984616279602
time_total_s: 347.19752192497253
timestamp: 1664969580
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

-- PPO Agents -- Steps done: 8
=>> PBS: job killed: walltime 601 exceeded limit 600
*** Aborted at 1664969690 (unix time) try "date -d @1664969690" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xfe7c) received by PID 63313 (TID 0x2ae761d033c0) from PID 65148; stack trace: ***
    @     0x2ae761efc630 (unknown)
    @     0x2ae76290abf9 syscall
2022-10-05 13:34:50,219	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
2022-10-05 13:34:50,222	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
    @     0x2ae7aa954479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2ae7aa953ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2ae7aa9510e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2ae7aa9515e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2ae7a99cb40b tensorflow::DirectSession::RunInternal()
    @     0x2ae7a99cc829 tensorflow::DirectSession::Run()
    @     0x2ae7a99b5134 tensorflow::DirectSession::Run()
    @     0x2ae799cd08c2 tensorflow::SessionRef::Run()
    @     0x2ae79a12bf5e TF_Run_Helper()
    @     0x2ae79a12cc28 TF_SessionRun
    @     0x2ae799cca01f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2ae799cca0c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2ae7cd846274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2ae7cd82fc98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
