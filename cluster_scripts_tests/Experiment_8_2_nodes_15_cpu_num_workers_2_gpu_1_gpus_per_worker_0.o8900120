HEAD NODE:  thishost
Allocate Nodes = <hpc-g04-node02 hpc-g04-node01>
set up ray cluster...


Working with node hpc-g04-node02
first allocate node - use as headnode ...
2022-10-05 13:35:22,065	INFO scripts.py:357 -- Using IP address 192.168.115.44 for this node.
2022-10-05 13:35:22,068	INFO resource_spec.py:212 -- Starting Ray with 406.15 GiB memory available for workers and up to 178.06 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 13:35:22,965	INFO services.py:1170 -- View the Ray dashboard at [1m[32m192.168.115.44:8265[39m[22m
2022-10-05 13:35:23,008	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.44:3679' --redis-password='2bffd9eb-eb88-4b61-8ea9-b59577c47420'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='2bffd9eb-eb88-4b61-8ea9-b59577c47420')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-g04-node01
then allocate other nodes:  1
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.44:3679
2022-10-05 13:35:30,814	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-05 13:35:30,839	INFO resource_spec.py:212 -- Starting Ray with 529.44 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-05 13:35:30,863	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

done, now launching python program
Inside covid19_components.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
Inside covid19_env.py: 1 GPUs are available.
Warning: The 'WarpDrive' package is not found and cannot be used! If you wish to use WarpDrive, please run 'pip install rl-warp-drive' first.
2022-10-05 13:37:01,606	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-05 13:37:01,635 seed (final): 27741000
2022-10-05 13:37:01,697	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-05 13:37:01,917	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-05 13:37:24,144	INFO trainable.py:180 -- _setup took 22.228 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 13:37:24,144	INFO trainable.py:217 -- Getting current IP.
2022-10-05 13:37:37,645	INFO trainable.py:180 -- _setup took 13.391 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-05 13:37:37,645	INFO trainable.py:217 -- Getting current IP.
2022-10-05 13:37:37,674 Not restoring trainer...
2022-10-05 13:37:37,675 Starting with fresh agent TF weights.
2022-10-05 13:37:37,675 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-05 13:40:52,351 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-05 13:40:52,356 custom_metrics: {}
date: 2022-10-05_13-40-52
done: false
episode_len_mean: 1000.0
episode_reward_max: 90.34790623573369
episode_reward_mean: 76.70530301215848
episode_reward_min: 63.77587941868688
episodes_this_iter: 4
episodes_total: 4
experiment_id: 4648ecbb4bc747b6b55b15145410318a
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 141088.437
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.781105399131775
      entropy_coeff: 0.02500000037252903
      kl: 0.09694697707891464
      model: {}
      policy_loss: -0.13890251517295837
      total_loss: -0.15722720324993134
      vf_explained_var: 0.8568466305732727
      vf_loss: 0.5240585803985596
  load_time_ms: 1485.768
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 44198.839
  update_time_ms: 6758.965
iterations_since_restore: 1
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 37.14712643678161
  gpu_util_percent0: 0.0
  ram_util_percent: 19.043678160919537
  vram_util_percent0: 0.018997524752475244
pid: 68001
policy_reward_max:
  agent_policy: 24.973326536732078
  planner_policy: 21.19958432085763
policy_reward_mean:
  agent_policy: 14.601404630165375
  planner_policy: 18.29968449149729
policy_reward_min:
  agent_policy: -1.9783118582457848
  planner_policy: 13.832486978463107
sampler_perf:
  mean_env_wait_ms: 2.0985161882826593
  mean_inference_ms: 2.7032061733644284
  mean_processing_ms: 0.6495896486685552
time_since_restore: 193.81317973136902
time_this_iter_s: 193.81317973136902
time_total_s: 193.81317973136902
timestamp: 1664970052
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-05 13:40:52,526 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
2022-10-05 13:43:25,585 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-05 13:43:25,590 custom_metrics: {}
date: 2022-10-05_13-43-25
done: false
episode_len_mean: 1000.0
episode_reward_max: 169.52448063757544
episode_reward_mean: 89.57934671180078
episode_reward_min: 63.77587941868688
episodes_this_iter: 4
episodes_total: 8
experiment_id: 4648ecbb4bc747b6b55b15145410318a
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 141240.298
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.71564519405365
      entropy_coeff: 0.02500000037252903
      kl: 0.12527772784233093
      model: {}
      policy_loss: -0.16865558922290802
      total_loss: -0.19500985741615295
      vf_explained_var: 0.9379358291625977
      vf_loss: 0.3307369649410248
  load_time_ms: 1372.839
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 27269.741
  update_time_ms: 3382.485
iterations_since_restore: 2
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 35.52780748663101
  gpu_util_percent0: 0.0
  ram_util_percent: 18.776470588235295
  vram_util_percent0: 0.018997524752475244
pid: 68001
policy_reward_max:
  agent_policy: 57.16295073548421
  planner_policy: 41.37765921351085
policy_reward_mean:
  agent_policy: 17.193895904248247
  planner_policy: 20.80376309480763
policy_reward_min:
  agent_policy: -4.944592179241677
  planner_policy: 13.217768438175781
sampler_perf:
  mean_env_wait_ms: 2.065632325568977
  mean_inference_ms: 2.6304234218768503
  mean_processing_ms: 0.646606573297519
time_since_restore: 346.85005044937134
time_this_iter_s: 153.03687071800232
time_total_s: 346.85005044937134
timestamp: 1664970205
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

-- PPO Agents -- Steps done: 8
2022-10-05 13:46:07,284 Iter 3: steps this-iter 4000 total 12000 -> 12/5000 episodes done
2022-10-05 13:46:07,336 custom_metrics: {}
date: 2022-10-05_13-46-07
done: false
episode_len_mean: 1000.0
episode_reward_max: 169.52448063757544
episode_reward_mean: 113.57038933239389
episode_reward_min: 63.77587941868688
episodes_this_iter: 4
episodes_total: 12
experiment_id: 4648ecbb4bc747b6b55b15145410318a
hostname: hpc-g04-node02.unitn.it
info:
  grad_time_ms: 144094.993
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7765520811080933
      entropy_coeff: 0.02500000037252903
      kl: 0.14414173364639282
      model: {}
      policy_loss: -0.18853195011615753
      total_loss: -0.21181249618530273
      vf_explained_var: 0.9517832398414612
      vf_loss: 0.42266520857810974
  load_time_ms: 1285.988
  num_steps_sampled: 12000
  num_steps_trained: 48000
  sample_time_ms: 21713.204
  update_time_ms: 2287.832
iterations_since_restore: 3
node_ip: 192.168.115.44
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 56.9548717948718
  gpu_util_percent0: 0.0
  ram_util_percent: 21.71589743589744
  vram_util_percent0: 0.01899752475247524
pid: 68001
policy_reward_max:
  agent_policy: 96.30555232669889
  planner_policy: 46.631496436526305
policy_reward_mean:
  agent_policy: 22.16365195577463
  planner_policy: 24.915781509294654
policy_reward_min:
  agent_policy: -6.713478146438338
  planner_policy: 13.217768438175781
sampler_perf:
  mean_env_wait_ms: 2.043327936731938
  mean_inference_ms: 2.5828458631371634
  mean_processing_ms: 0.6483524600173183
time_since_restore: 508.5181760787964
time_this_iter_s: 161.66812562942505
time_total_s: 508.5181760787964
timestamp: 1664970367
timesteps_since_restore: 12000
timesteps_this_iter: 4000
timesteps_total: 12000
training_iteration: 3

-- PPO Agents -- Steps done: 12
=>> PBS: job killed: walltime 688 exceeded limit 600
*** Aborted at 1664970408 (unix time) try "date -d @1664970408" if you are using GNU date ***
2022-10-05 13:46:48,546	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
2022-10-05 13:46:48,619	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
PC: @                0x0 (unknown)
*** SIGTERM (@0xfe7c) received by PID 68001 (TID 0x2ac923cb83c0) from PID 65148; stack trace: ***
    @     0x2ac923eb1630 (unknown)
    @     0x2ac9248bfbf9 syscall
    @     0x2ac96c909479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2ac96c908ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2ac96c9060e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2ac96c9065e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2ac96b98040b tensorflow::DirectSession::RunInternal()
    @     0x2ac96b981829 tensorflow::DirectSession::Run()
    @     0x2ac96b96a134 tensorflow::DirectSession::Run()
    @     0x2ac95bc858c2 tensorflow::SessionRef::Run()
    @     0x2ac95c0e0f5e TF_Run_Helper()
    @     0x2ac95c0e1c28 TF_SessionRun
    @     0x2ac95bc7f01f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2ac95bc7f0c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2ac98f7fb274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2ac98f7e4c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
