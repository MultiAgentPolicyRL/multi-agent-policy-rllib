Allocate Nodes = <hpc-c11-node15 hpc-c11-node07 hpc-c10-node01 hpc-c11-node11>
set up ray cluster...


Working with node hpc-c11-node15
first allocate node - use as headnode ...
2022-10-04 18:33:15,032	INFO scripts.py:357 -- Using IP address 192.168.115.81 for this node.
2022-10-04 18:33:15,045	INFO resource_spec.py:212 -- Starting Ray with 760.55 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:33:15,923	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 18:33:15,963	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.81:6379' --redis-password='30a6a19f-8909-4d1b-a115-a9fcfda3b456'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='30a6a19f-8909-4d1b-a115-a9fcfda3b456')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c11-node07
then allocate other nodes:  1
node NAME: hpc-c11-node07.unitn.it
node IP: 192.168.115.43
dest IP: 192.168.115.81:6379
2022-10-04 18:33:28,411	INFO scripts.py:429 -- Using IP address 192.168.115.43 for this node.
2022-10-04 18:33:28,415	INFO resource_spec.py:212 -- Starting Ray with 318.41 GiB memory available for workers and up to 136.46 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:33:28,441	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c10-node01
then allocate other nodes:  2
node NAME: hpc-c10-node01.unitn.it
node IP: 192.168.115.134
dest IP: 192.168.115.81:6379
2022-10-04 18:33:39,251	INFO scripts.py:429 -- Using IP address 192.168.115.134 for this node.
2022-10-04 18:33:39,257	INFO resource_spec.py:212 -- Starting Ray with 290.97 GiB memory available for workers and up to 124.71 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:33:39,373	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node07.unitn.it
exiting hpc-c10-node01.unitn.it

Working with node hpc-c11-node11
then allocate other nodes:  3
node NAME: hpc-c11-node11.unitn.it
node IP: 192.168.115.78
dest IP: 192.168.115.81:6379
2022-10-04 18:33:46,980	INFO scripts.py:429 -- Using IP address 192.168.115.78 for this node.
2022-10-04 18:33:46,989	INFO resource_spec.py:212 -- Starting Ray with 662.35 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 18:33:47,012	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node11.unitn.it

done, now launching python program
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 18:35:12,148	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 18:35:12,176 seed (final): 24768000
2022-10-04 18:35:12,235	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 18:35:12,458	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 18:35:26,519	WARNING worker.py:1090 -- The actor or task with ID ffffffffffffffff45b95b1c0100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:192.168.115.81: 1.000000}, {CPU: 96.000000}, {memory: 760.546875 GiB}, {object_store_memory: 128.515625 GiB}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2022-10-04 18:35:35,339	INFO trainable.py:180 -- _setup took 22.882 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:35:35,339	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:35:51,266	INFO trainable.py:180 -- _setup took 15.849 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 18:35:51,266	INFO trainable.py:217 -- Getting current IP.
2022-10-04 18:35:51,270 Not restoring trainer...
2022-10-04 18:35:51,270 Starting with fresh agent TF weights.
2022-10-04 18:35:51,270 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 18:39:20,974 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-04 18:39:20,980 custom_metrics: {}
date: 2022-10-04_18-39-20
done: false
episode_len_mean: 1000.0
episode_reward_max: 108.70131950144702
episode_reward_mean: 91.96476905626677
episode_reward_min: 80.64477256852525
episodes_this_iter: 4
episodes_total: 4
experiment_id: ddd1473e79644732b48f265b55d253bd
hostname: hpc-c11-node15.unitn.it
info:
  grad_time_ms: 151266.125
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.6733213663101196
      entropy_coeff: 0.02500000037252903
      kl: 0.11096137762069702
      model: {}
      policy_loss: -0.15220002830028534
      total_loss: -0.17339201271533966
      vf_explained_var: 0.8676307797431946
      vf_loss: 0.41282132267951965
  load_time_ms: 1251.669
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 47962.155
  update_time_ms: 7805.579
iterations_since_restore: 1
node_ip: 192.168.115.81
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 67.09486301369861
  ram_util_percent: 5.447602739726027
pid: 71812
policy_reward_max:
  agent_policy: 50.0257977708889
  planner_policy: 22.565835308381246
policy_reward_mean:
  agent_policy: 19.45026271689713
  planner_policy: 14.163718188678578
policy_reward_min:
  agent_policy: -2.3563492357920515
  planner_policy: 8.92237077966218
sampler_perf:
  mean_env_wait_ms: 2.1281793438035925
  mean_inference_ms: 2.9644627144549975
  mean_processing_ms: 0.6471979564455138
time_since_restore: 208.70792937278748
time_this_iter_s: 208.70792937278748
time_total_s: 208.70792937278748
timestamp: 1664901560
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 18:39:21,149 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 4
2022-10-04 18:41:56,414 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-04 18:41:56,419 custom_metrics: {}
date: 2022-10-04_18-41-56
done: false
episode_len_mean: 1000.0
episode_reward_max: 362.14188638869143
episode_reward_mean: 168.9966299616571
episode_reward_min: 80.64477256852525
episodes_this_iter: 4
episodes_total: 8
experiment_id: ddd1473e79644732b48f265b55d253bd
hostname: hpc-c11-node15.unitn.it
info:
  grad_time_ms: 146973.601
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.842313289642334
      entropy_coeff: 0.02500000037252903
      kl: 0.13890036940574646
      model: {}
      policy_loss: -0.19093157351016998
      total_loss: -0.19909203052520752
      vf_explained_var: 0.9178052544593811
      vf_loss: 0.7579473257064819
  load_time_ms: 1012.163
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 29808.365
  update_time_ms: 3947.379
iterations_since_restore: 2
node_ip: 192.168.115.81
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 68.4548223350254
  ram_util_percent: 5.6208121827411155
pid: 71812
policy_reward_max:
  agent_policy: 129.01028830277596
  planner_policy: 109.39752337714278
policy_reward_mean:
  agent_policy: 31.29699900781876
  planner_policy: 43.80863393038239
policy_reward_min:
  agent_policy: -2.3563492357920515
  planner_policy: 8.92237077966218
sampler_perf:
  mean_env_wait_ms: 2.1009233577960424
  mean_inference_ms: 2.9186401609771075
  mean_processing_ms: 0.6476097080422857
time_since_restore: 363.9492030143738
time_this_iter_s: 155.2412736415863
time_total_s: 363.9492030143738
timestamp: 1664901716
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

-- PPO Agents -- Steps done: 8
=>> PBS: job killed: walltime 602 exceeded limit 600
*** Aborted at 1664901795 (unix time) try "date -d @1664901795" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xd4b) received by PID 71812 (TID 0x2aeac5ae83c0) from PID 3403; stack trace: ***
    @     0x2aeac5ce6630 (unknown)
    @     0x2aeac66f4e29 syscall
    @     0x2aeb1913f479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2aeb1913eab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2aeb1913c0e3 nsync::nsync_cv_wait_with_deadline_generic()
2022-10-04 18:43:15,357	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
    @     0x2aeb1913c5e3 nsync::nsync_cv_wait_with_deadline()
2022-10-04 18:43:15,362	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
    @     0x2aeb181b640b tensorflow::DirectSession::RunInternal()
    @     0x2aeb181b7829 tensorflow::DirectSession::Run()
    @     0x2aeb181a0134 tensorflow::DirectSession::Run()
    @     0x2aeb084bb8c2 tensorflow::SessionRef::Run()
    @     0x2aeb08916f5e TF_Run_Helper()
    @     0x2aeb08917c28 TF_SessionRun
    @     0x2aeb084b501f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2aeb084b50c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2aeb3bd98274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2aeb3bd81c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
