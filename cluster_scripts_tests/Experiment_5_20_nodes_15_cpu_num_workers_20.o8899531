Allocate Nodes = <hpc-c11-node15 hpc-c11-node11 hpc-c07-node12 hpc-g04-node01 hpc-c03-node24 hpc-c05-node03 hpc-c03-node12 hpc-c11-node07 hpc-c11-node08 hpc-c03-node17 hpc-c10-node01>
set up ray cluster...


Working with node hpc-c11-node15
first allocate node - use as headnode ...
2022-10-04 20:08:17,540	INFO scripts.py:357 -- Using IP address 192.168.115.81 for this node.
2022-10-04 20:08:17,551	INFO resource_spec.py:212 -- Starting Ray with 789.4 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:08:18,429	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-04 20:08:18,465	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.81:3679' --redis-password='34d7dbbc-0c9b-48e6-995b-25391ee06b1f'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='34d7dbbc-0c9b-48e6-995b-25391ee06b1f')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop

Working with node hpc-c11-node11
then allocate other nodes:  1
node NAME: hpc-c11-node15.unitn.it
node IP: 192.168.115.81
dest IP: 192.168.115.81:3679

Working with node hpc-c07-node12
then allocate other nodes:  2
node NAME: hpc-c11-node11.unitn.it
node IP: 192.168.115.78
dest IP: 192.168.115.81:3679
2022-10-04 20:08:39,523	INFO scripts.py:429 -- Using IP address 192.168.115.78 for this node.
2022-10-04 20:08:39,534	INFO resource_spec.py:212 -- Starting Ray with 662.06 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:08:39,561	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node11.unitn.it

Working with node hpc-g04-node01
then allocate other nodes:  3
node NAME: hpc-c11-node11.unitn.it
node IP: 192.168.115.78
dest IP: 192.168.115.81:3679
2022-10-04 20:08:49,467	INFO scripts.py:429 -- Using IP address 192.168.115.78 for this node.
2022-10-04 20:08:49,476	INFO resource_spec.py:212 -- Starting Ray with 671.14 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:08:49,502	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node11.unitn.it

Working with node hpc-c03-node24
then allocate other nodes:  4
node NAME: hpc-c07-node12.unitn.it
node IP: 192.168.115.53
dest IP: 192.168.115.81:3679
2022-10-04 20:08:59,575	INFO scripts.py:429 -- Using IP address 192.168.115.53 for this node.
2022-10-04 20:08:59,578	INFO resource_spec.py:212 -- Starting Ray with 149.32 GiB memory available for workers and up to 64.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:08:59,611	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c07-node12.unitn.it

Working with node hpc-c05-node03
then allocate other nodes:  5
node NAME: hpc-g04-node01.unitn.it
node IP: 192.168.115.24
dest IP: 192.168.115.81:3679
2022-10-04 20:09:09,189	INFO scripts.py:429 -- Using IP address 192.168.115.24 for this node.
2022-10-04 20:09:09,192	INFO resource_spec.py:212 -- Starting Ray with 529.35 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:09:09,218	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-g04-node01.unitn.it

Working with node hpc-c03-node12
then allocate other nodes:  6
node NAME: hpc-c03-node24.unitn.it
node IP: 192.168.115.12
dest IP: 192.168.115.81:3679
2022-10-04 20:09:19,142	INFO scripts.py:429 -- Using IP address 192.168.115.12 for this node.
2022-10-04 20:09:19,145	INFO resource_spec.py:212 -- Starting Ray with 232.52 GiB memory available for workers and up to 99.65 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:09:19,179	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c03-node24.unitn.it

Working with node hpc-c11-node07
then allocate other nodes:  7
node NAME: hpc-c05-node03.unitn.it
node IP: 192.168.115.60
dest IP: 192.168.115.81:3679
2022-10-04 20:09:29,260	INFO scripts.py:429 -- Using IP address 192.168.115.60 for this node.
2022-10-04 20:09:29,263	INFO resource_spec.py:212 -- Starting Ray with 557.67 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:09:29,293	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c05-node03.unitn.it

Working with node hpc-c11-node08
then allocate other nodes:  8
node NAME: hpc-c05-node03.unitn.it
node IP: 192.168.115.60
dest IP: 192.168.115.81:3679
2022-10-04 20:09:39,262	INFO scripts.py:429 -- Using IP address 192.168.115.60 for this node.
2022-10-04 20:09:39,265	INFO resource_spec.py:212 -- Starting Ray with 557.62 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:09:39,296	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c05-node03.unitn.it
2022-10-04 20:09:45,752	INFO scripts.py:429 -- Using IP address 192.168.115.81 for this node.
2022-10-04 20:09:45,761	INFO resource_spec.py:212 -- Starting Ray with 790.09 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:09:45,795	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c11-node15.unitn.it

Working with node hpc-c03-node17
then allocate other nodes:  9
node NAME: hpc-c03-node12.unitn.it
node IP: 192.168.115.105
dest IP: 192.168.115.81:3679
2022-10-04 20:09:49,117	INFO scripts.py:429 -- Using IP address 192.168.115.105 for this node.
2022-10-04 20:09:49,120	INFO resource_spec.py:212 -- Starting Ray with 235.84 GiB memory available for workers and up to 101.09 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:09:49,153	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop
exiting hpc-c03-node12.unitn.it

Working with node hpc-c10-node01
then allocate other nodes:  10
node NAME: hpc-c11-node07.unitn.it
node IP: 192.168.115.43
dest IP: 192.168.115.81:3679
2022-10-04 20:10:00,738	INFO scripts.py:429 -- Using IP address 192.168.115.43 for this node.
2022-10-04 20:10:00,742	INFO resource_spec.py:212 -- Starting Ray with 318.6 GiB memory available for workers and up to 136.55 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-04 20:10:00,807	INFO scripts.py:438 -- 
Started Ray on this node. If you wish to terminate the processes that have been started, run

    ray stop

done, now launching python program
exiting hpc-c11-node07.unitn.it
Inside covid19_components.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
Inside covid19_env.py: 0 GPUs are available.
No GPUs found! Running the simulation on a CPU.
2022-10-04 20:10:30,772	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-04 20:10:30,803 seed (final): 30486000
2022-10-04 20:10:30,866	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-04 20:10:31,073	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-04 20:10:41,220	WARNING worker.py:1090 -- The node with node id 2025c16a5959152e9f695a6d271b346045720a86 has been marked dead because the detector has missed too many heartbeats from it.
2022-10-04 20:10:54,034	INFO trainable.py:180 -- _setup took 22.962 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 20:10:54,034	INFO trainable.py:217 -- Getting current IP.
2022-10-04 20:11:09,609	INFO trainable.py:180 -- _setup took 15.501 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-04 20:11:09,609	INFO trainable.py:217 -- Getting current IP.
2022-10-04 20:11:09,612 Not restoring trainer...
2022-10-04 20:11:09,613 Starting with fresh agent TF weights.
2022-10-04 20:11:09,613 Starting with fresh planner TF weights.
Training
-- PPO Agents -- Steps done: 0
2022-10-04 20:14:13,174 Iter 1: steps this-iter 4000 total 4000 -> 3/5000 episodes done
2022-10-04 20:14:13,180 custom_metrics: {}
date: 2022-10-04_20-14-13
done: false
episode_len_mean: 1000.0
episode_reward_max: 401.2231095613848
episode_reward_mean: 369.3267108180146
episode_reward_min: 311.29990023497487
episodes_this_iter: 3
episodes_total: 3
experiment_id: 55fa0d6e302349a3895364963ef50ee2
hostname: hpc-c11-node15.unitn.it
info:
  grad_time_ms: 140416.947
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 2.0938799381256104
      entropy_coeff: 0.02500000037252903
      kl: 0.12350523471832275
      model: {}
      policy_loss: -0.17861518263816833
      total_loss: -0.17054344713687897
      vf_explained_var: 0.9016531109809875
      vf_loss: 1.2083749771118164
  load_time_ms: 1145.587
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 32703.125
  update_time_ms: 7882.713
iterations_since_restore: 1
node_ip: 192.168.115.81
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 59.07821011673152
  ram_util_percent: 3.1307392996108954
pid: 15291
policy_reward_max:
  agent_policy: 139.54203182724677
  planner_policy: 136.12643830709754
policy_reward_mean:
  agent_policy: 65.03435930716624
  planner_policy: 109.18927358934802
policy_reward_min:
  agent_policy: 2.6248898079322682
  planner_policy: 84.35909446257726
sampler_perf:
  mean_env_wait_ms: 2.312450085744128
  mean_inference_ms: 3.502355504011268
  mean_processing_ms: 0.7145781615172151
time_since_restore: 182.53138542175293
time_this_iter_s: 182.53138542175293
time_total_s: 182.53138542175293
timestamp: 1664907253
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-04 20:14:13,371 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
-- PPO Agents -- Steps done: 3
2022-10-04 20:16:44,223 Iter 2: steps this-iter 4000 total 8000 -> 7/5000 episodes done
2022-10-04 20:16:44,228 custom_metrics: {}
date: 2022-10-04_20-16-44
done: false
episode_len_mean: 1000.0
episode_reward_max: 421.3350250687886
episode_reward_mean: 351.95137844924193
episode_reward_min: 253.84127627605508
episodes_this_iter: 4
episodes_total: 7
experiment_id: 55fa0d6e302349a3895364963ef50ee2
hostname: hpc-c11-node15.unitn.it
info:
  grad_time_ms: 138643.039
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9538934230804443
      entropy_coeff: 0.02500000037252903
      kl: 0.1405014991760254
      model: {}
      policy_loss: -0.20424379408359528
      total_loss: -0.20875371992588043
      vf_explained_var: 0.9368724822998047
      vf_loss: 0.8867475986480713
  load_time_ms: 889.111
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 22998.709
  update_time_ms: 3946.43
iterations_since_restore: 2
node_ip: 192.168.115.81
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 59.165625
  ram_util_percent: 3.210416666666667
pid: 15291
policy_reward_max:
  agent_policy: 139.54203182724677
  planner_policy: 136.59614348306312
policy_reward_mean:
  agent_policy: 61.21899449977872
  planner_policy: 107.07540045012634
policy_reward_min:
  agent_policy: 2.6248898079322682
  planner_policy: 60.162702718220366
sampler_perf:
  mean_env_wait_ms: 2.3162356745467414
  mean_inference_ms: 3.435097652817838
  mean_processing_ms: 0.7333298826426464
time_since_restore: 333.3635594844818
time_this_iter_s: 150.83217406272888
time_total_s: 333.3635594844818
timestamp: 1664907404
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

2022-10-04 20:16:44,416 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000008000
-- PPO Agents -- Steps done: 7
2022-10-04 20:19:08,360 Iter 3: steps this-iter 4000 total 12000 -> 11/5000 episodes done
2022-10-04 20:19:08,365 custom_metrics: {}
date: 2022-10-04_20-19-08
done: false
episode_len_mean: 1000.0
episode_reward_max: 421.3350250687886
episode_reward_mean: 339.2104249779373
episode_reward_min: 217.43482712185346
episodes_this_iter: 4
episodes_total: 11
experiment_id: 55fa0d6e302349a3895364963ef50ee2
hostname: hpc-c11-node15.unitn.it
info:
  grad_time_ms: 135796.119
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.891245722770691
      entropy_coeff: 0.02500000037252903
      kl: 0.16286064684391022
      model: {}
      policy_loss: -0.20678670704364777
      total_loss: -0.19925934076309204
      vf_explained_var: 0.9450981020927429
      vf_loss: 1.0961700677871704
  load_time_ms: 924.306
  num_steps_sampled: 12000
  num_steps_trained: 48000
  sample_time_ms: 19588.7
  update_time_ms: 2634.441
iterations_since_restore: 3
node_ip: 192.168.115.81
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 59.03480662983426
  ram_util_percent: 3.274585635359117
pid: 15291
policy_reward_max:
  agent_policy: 139.54203182724677
  planner_policy: 142.2968312699344
policy_reward_mean:
  agent_policy: 58.313457770723716
  planner_policy: 105.95659389504198
policy_reward_min:
  agent_policy: 2.6248898079322682
  planner_policy: 49.94881293927978
sampler_perf:
  mean_env_wait_ms: 2.3050893700644823
  mean_inference_ms: 3.3760442982019243
  mean_processing_ms: 0.735940142546475
time_since_restore: 477.2873020172119
time_this_iter_s: 143.9237425327301
time_total_s: 477.2873020172119
timestamp: 1664907548
timesteps_since_restore: 12000
timesteps_this_iter: 4000
timesteps_total: 12000
training_iteration: 3

-- PPO Agents -- Steps done: 11
=>> PBS: job killed: walltime 686 exceeded limit 600
*** Aborted at 1664907582 (unix time) try "date -d @1664907582" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGTERM (@0xd4b) received by PID 15291 (TID 0x2ac668e323c0) from PID 3403; stack trace: ***
    @     0x2ac669030630 (unknown)
    @     0x2ac669a3ee29 syscall
2022-10-04 20:19:42,062	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
2022-10-04 20:19:42,070	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
    @     0x2ac6b1a84479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2ac6b1a83ab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2ac6b1a810e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2ac6b1a815e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2ac6b0afb40b tensorflow::DirectSession::RunInternal()
    @     0x2ac6b0afc829 tensorflow::DirectSession::Run()
    @     0x2ac6b0ae5134 tensorflow::DirectSession::Run()
    @     0x2ac6a0e008c2 tensorflow::SessionRef::Run()
    @     0x2ac6a125bf5e TF_Run_Helper()
    @     0x2ac6a125cc28 TF_SessionRun
    @     0x2ac6a0dfa01f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2ac6a0dfa0c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2ac6d46dd274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2ac6d46c6c98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
