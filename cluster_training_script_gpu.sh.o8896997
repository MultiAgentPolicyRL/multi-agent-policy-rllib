Allocate Nodes = <hpc-g04-node01>
first allocate node - use as headnode ...
2022-10-03 17:01:19,842	INFO scripts.py:357 -- Using IP address 192.168.115.24 for this node.
2022-10-03 17:01:19,845	INFO resource_spec.py:212 -- Starting Ray with 518.46 GiB memory available for workers and up to 186.26 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2022-10-03 17:01:20,599	INFO services.py:1170 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2022-10-03 17:01:20,639	INFO scripts.py:387 -- 
Started Ray on this node. You can add additional nodes to the cluster by calling

    ray start --address='192.168.115.24:6379' --redis-password='33ff26a2-4a58-4b13-9cdc-32ba674e58cf'

from the node you wish to add. You can connect a driver to the cluster from Python by running

    import ray
    ray.init(address='auto', redis_password='33ff26a2-4a58-4b13-9cdc-32ba674e58cf')

If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run

    ray stop
2022-10-03 17:01:51,205	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
2022-10-03 17:01:51,228 seed (final): 63839000
2022-10-03 17:01:51,286	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2022-10-03 17:01:51,502	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-10-03 17:02:13,533	INFO trainable.py:180 -- _setup took 22.033 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-03 17:02:13,533	INFO trainable.py:217 -- Getting current IP.
2022-10-03 17:02:27,151	INFO trainable.py:180 -- _setup took 13.511 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-03 17:02:27,151	INFO trainable.py:217 -- Getting current IP.
2022-10-03 17:02:27,170 Not restoring trainer...
2022-10-03 17:02:27,172 Starting with fresh agent TF weights.
2022-10-03 17:02:27,172 Starting with fresh planner TF weights.
2022-10-03 17:05:36,012 Iter 1: steps this-iter 4000 total 4000 -> 4/5000 episodes done
2022-10-03 17:05:36,019 custom_metrics: {}
date: 2022-10-03_17-05-35
done: false
episode_len_mean: 1000.0
episode_reward_max: 290.8585436880308
episode_reward_mean: 252.74673700630973
episode_reward_min: 225.83600892059064
episodes_this_iter: 4
episodes_total: 4
experiment_id: 03d31600dca84e17819c121c288e735d
hostname: hpc-g04-node01.unitn.it
info:
  grad_time_ms: 149531.015
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 2.040205240249634
      entropy_coeff: 0.02500000037252903
      kl: 0.11079554259777069
      model: {}
      policy_loss: -0.1814873367547989
      total_loss: -0.18034863471984863
      vf_explained_var: 0.8809577226638794
      vf_loss: 1.0428763628005981
  load_time_ms: 972.422
  num_steps_sampled: 4000
  num_steps_trained: 16000
  sample_time_ms: 30038.027
  update_time_ms: 7030.115
iterations_since_restore: 1
node_ip: 192.168.115.24
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 80.71865671641791
  gpu_util_percent0: 0.0
  ram_util_percent: 2.0220149253731345
  vram_util_percent0: 0.018997524752475244
pid: 32719
policy_reward_max:
  agent_policy: 66.88091863720977
  planner_policy: 106.07168937758865
policy_reward_mean:
  agent_policy: 42.521147450214016
  planner_policy: 82.66214720545418
policy_reward_min:
  agent_policy: 14.576651109534042
  planner_policy: 59.827511350647185
sampler_perf:
  mean_env_wait_ms: 2.2570831307406904
  mean_inference_ms: 2.661576871571691
  mean_processing_ms: 0.676863494960741
time_since_restore: 187.99031376838684
time_this_iter_s: 187.99031376838684
time_total_s: 187.99031376838684
timestamp: 1664809535
timesteps_since_restore: 4000
timesteps_this_iter: 4000
timesteps_total: 4000
training_iteration: 1

2022-10-03 17:05:36,212 >> Wrote dense logs to: /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/dense_logs/logs_0000000000004000
2022-10-03 17:08:15,824 Iter 2: steps this-iter 4000 total 8000 -> 8/5000 episodes done
2022-10-03 17:08:15,831 custom_metrics: {}
date: 2022-10-03_17-08-15
done: false
episode_len_mean: 1000.0
episode_reward_max: 312.8180796695202
episode_reward_mean: 253.29860072608795
episode_reward_min: 200.77730836191205
episodes_this_iter: 4
episodes_total: 8
experiment_id: 03d31600dca84e17819c121c288e735d
hostname: hpc-g04-node01.unitn.it
info:
  grad_time_ms: 148830.706
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8706976175308228
      entropy_coeff: 0.02500000037252903
      kl: 0.1285625398159027
      model: {}
      policy_loss: -0.18708747625350952
      total_loss: -0.2048090547323227
      vf_explained_var: 0.9448984265327454
      vf_loss: 0.580917239189148
  load_time_ms: 729.661
  num_steps_sampled: 8000
  num_steps_trained: 32000
  sample_time_ms: 20486.172
  update_time_ms: 3519.833
iterations_since_restore: 2
node_ip: 192.168.115.24
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 82.23809523809524
  gpu_util_percent0: 0.0
  ram_util_percent: 2.0928571428571434
  vram_util_percent0: 0.018997524752475244
pid: 32719
policy_reward_max:
  agent_policy: 98.02783983961402
  planner_policy: 106.07168937758865
policy_reward_mean:
  agent_policy: 43.29875025550588
  planner_policy: 80.10359970406435
policy_reward_min:
  agent_policy: 4.2074020449580845
  planner_policy: 40.40313487417824
sampler_perf:
  mean_env_wait_ms: 2.228675994810966
  mean_inference_ms: 2.6217020632797365
  mean_processing_ms: 0.6775178789149648
time_since_restore: 347.57797360420227
time_this_iter_s: 159.58765983581543
time_total_s: 347.57797360420227
timestamp: 1664809695
timesteps_since_restore: 8000
timesteps_this_iter: 4000
timesteps_total: 8000
training_iteration: 2

2022-10-03 17:08:16,149 Saved Trainer snapshot + Env object @ /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/ckpts/latest_checkpoint.agent.pkl
2022-10-03 17:08:16,155 Saved TF weights @ /home/ettore.saggiorato/ai-economist-ppo-decision-tree/ai-economist/tutorials/rllib/experiments/check/phase1_gpu/ckpts/agent.tf.weights.global-step-8000
2022-10-03 17:08:16,155 Checkpoint saved @ step 8000
2022-10-03 17:10:56,659 Iter 3: steps this-iter 4000 total 12000 -> 12/5000 episodes done
2022-10-03 17:10:56,665 custom_metrics: {}
date: 2022-10-03_17-10-56
done: false
episode_len_mean: 1000.0
episode_reward_max: 342.1699265096333
episode_reward_mean: 248.0544947099731
episode_reward_min: 138.4957970227878
episodes_this_iter: 4
episodes_total: 12
experiment_id: 03d31600dca84e17819c121c288e735d
hostname: hpc-g04-node01.unitn.it
info:
  grad_time_ms: 148892.989
  learner:
    agent_policy:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8603477478027344
      entropy_coeff: 0.02500000037252903
      kl: 0.14200109243392944
      model: {}
      policy_loss: -0.19895948469638824
      total_loss: -0.21204307675361633
      vf_explained_var: 0.949786901473999
      vf_loss: 0.6685023903846741
  load_time_ms: 669.947
  num_steps_sampled: 12000
  num_steps_trained: 48000
  sample_time_ms: 17286.573
  update_time_ms: 2348.935
iterations_since_restore: 3
node_ip: 192.168.115.24
num_healthy_workers: 2
off_policy_estimator: {}
optimizer_steps_this_iter: 1
perf:
  cpu_util_percent: 82.20478468899523
  gpu_util_percent0: 0.0
  ram_util_percent: 2.0928229665071774
  vram_util_percent0: 0.018997524752475244
pid: 32719
policy_reward_max:
  agent_policy: 139.9660149188304
  planner_policy: 106.07168937758865
policy_reward_mean:
  agent_policy: 42.84261910478954
  planner_policy: 76.68401829081505
policy_reward_min:
  agent_policy: 3.552423483222693
  planner_policy: 34.07925465435846
sampler_perf:
  mean_env_wait_ms: 2.214817971739775
  mean_inference_ms: 2.598687228733386
  mean_processing_ms: 0.6776928633781397
time_since_restore: 508.0583801269531
time_this_iter_s: 160.48040652275085
time_total_s: 508.0583801269531
timestamp: 1664809856
timesteps_since_restore: 12000
timesteps_this_iter: 4000
timesteps_total: 12000
training_iteration: 3

=>> PBS: job killed: walltime 688 exceeded limit 600
*** Aborted at 1664809966 (unix time) try "date -d @1664809966" if you are using GNU date ***
PC: @                0x0 (unknown)
2022-10-03 17:12:46,450	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
2022-10-03 17:12:46,451	ERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.
*** SIGTERM (@0xe22) received by PID 32719 (TID 0x2b5f8867a3c0) from PID 3618; stack trace: ***
    @     0x2b5f88873630 (unknown)
    @     0x2b5f89281bf9 syscall
    @     0x2b5fb9cc0479 nsync::nsync_mu_semaphore_p_with_deadline()
    @     0x2b5fb9cbfab9 nsync::nsync_sem_wait_with_cancel_()
    @     0x2b5fb9cbd0e3 nsync::nsync_cv_wait_with_deadline_generic()
    @     0x2b5fb9cbd5e3 nsync::nsync_cv_wait_with_deadline()
    @     0x2b5fb8d3740b tensorflow::DirectSession::RunInternal()
    @     0x2b5fb8d38829 tensorflow::DirectSession::Run()
    @     0x2b5fb8d21134 tensorflow::DirectSession::Run()
    @     0x2b5fa903c8c2 tensorflow::SessionRef::Run()
    @     0x2b5fa9497f5e TF_Run_Helper()
    @     0x2b5fa9498c28 TF_SessionRun
    @     0x2b5fa903601f tensorflow::TF_SessionRun_wrapper_helper()
    @     0x2b5fa90360c2 tensorflow::TF_SessionRun_wrapper()
    @     0x2b5fdcbb2274 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EEUlP10TF_SessionP9TF_BufferRKNS_6handleERKSt6vectorI9TF_OutputSaISC_EERKSB_IP12TF_OperationSaISI_EES7_E15_NS_6objectEJS5_S7_SA_SG_SM_S7_EJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES15_
    @     0x2b5fdcb9bc98 pybind11::cpp_function::dispatcher()
    @           0x46299c _PyCFunction_FastCallKeywords
    @           0x4fc23a _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x468f5f PyObject_Call
    @           0x4f91a4 _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501ed8 _PyEval_EvalCodeWithName
    @           0x4625aa _PyFunction_FastCallKeywords
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x46217a function_code_fastcall
    @           0x4f79ff _PyEval_EvalFrameDefault
    @           0x501a88 _PyEval_EvalCodeWithName
